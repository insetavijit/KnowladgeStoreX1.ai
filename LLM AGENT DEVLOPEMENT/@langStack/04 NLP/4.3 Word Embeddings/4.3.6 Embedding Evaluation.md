| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.3.6 Embedding Evaluation Overview]]**| Measure the quality of word vectors                            | intrinsic vs extrinsic evaluation, qualitative vs quantitative, benchmarks                 | How do you know if your vectors are any good?                          |
| **[[Intrinsic Evaluation]]**            | Test embeddings based on their internal properties             | analogy tasks (A:B :: C:D), similarity correlation (WordSim-353), outlier detection        | Checking if "King" is close to "Queen" without running a real application. |
| **[[Extrinsic Evaluation]]**            | Test embeddings on a downstream task                           | sentiment analysis accuracy, NER scores, impact on model convergence speed                 | Plugging the vectors into a real model to see if performance improves. |
| **[[WordSim-353 / SimLex-999]]**        | Standard benchmark datasets                                    | human judgment correlation, similarity vs relatedness ("coffee"~"cup" vs "coffee"~"tea")   | Comparing model scores against human intuition scores.                 |
| **[[Analogy Tasks]]**                   | Assessing algebraic structure                                  | Google Analogy Test Set, semantic analogies (states-capitals), syntactic (verb tenses)     | Can the model solve "Paris is to France as Tokyo is to...?"            |
| **[[Stability & Reliability]]**         | Consistency across training runs                               | random seed sensitivity, vocabulary overlap, robustness to hyperparameter changes          | Ensuring your embeddings aren't just a lucky roll of the dice.         |
