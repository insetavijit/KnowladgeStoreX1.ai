| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.3.5 FastText Overview]]**         | Enrich vectors with subword information                        | extension of Word2Vec (Skip-gram), character n-grams, handling OOV words                   | FastText looks inside the word, seeing "apple" as <ap, app, ppl, ple, le>. |
| **[[Character N-grams]]**               | The core innovation                                            | bag of character n-grams, 3-6 grams, summing n-gram vectors to get word vector             | Breaking words into shards allows sharing meaning between "eat", "eating", and "eater". |
| **[[OOV Capability]]**                  | Handling Out-Of-Vocabulary words                               | computing vectors for unseen words by summing their n-grams, no UNK token needed           | FastText can guess the meaning of a typo or a new word based on its parts. |
| **[[Morphological Richness]]**          | Why it excels at certain languages                             | German, Turkish, Russian advantages, capturing roots and affixes implicitly                | For agglutinative languages, FastText is superior to Word2Vec/GloVe.   |
| **[[Supervised FastText]]**             | Text classification usage                                      | `fasttext classifier`, extremely fast training, hierarchical softmax, average embedding    | FastText is also a blazing fast baseline for text classification.      |
| **[[Model Compression]]**               | Reducing size for mobile/web                                   | quantization, pruning n-grams, product quantization                                        | Compressing gigabytes of vectors into megabytes with minimal loss.     |
