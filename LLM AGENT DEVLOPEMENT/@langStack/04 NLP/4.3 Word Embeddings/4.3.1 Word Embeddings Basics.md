| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.3.1 Word Embeddings Basics Overview]]**| Map words to dense vector spaces                               | dimensionality reduction, semantic similarity, distributed representation, feature density | Embeddings turn words into coordinates where similar meanings are close neighbors. |
| **[[Sparse vs Dense Vectors]]**         | Contrast one-hot encoding with embeddings                      | high-dimensional/sparse (one-hot) vs low-dimensional/dense (embedding), memory efficiency  | One-hot is a lonely island; embeddings are a connected continent.      |
| **[[Distributional Hypothesis]]**       | The core linguistic theory behind embeddings                   | "You shall know a word by the company it keeps", context window, co-occurrence             | Words that appear in the same contexts likely mean similar things.     |
| **[[Vector Arithmetic]]**               | Perform algebraic operations on meaning                        | King - Man + Woman = Queen, analogy completion, semantic composition                       | You can do math with meaning.                                          |
| **[[Cosine Similarity]]**               | Measure the distance between word vectors                      | angle based distance, normalization, range [-1, 1], euclidean distance vs cosine           | Cosine similarity tells you how "parallel" two concepts are.           |
| **[[Static vs Contextual]]**            | Differentiate fixed vectors from dynamic ones                  | static (Word2Vec/GloVe) = one vector per word; contextual (BERT) = vector depends on sentence| Static embeddings are a dictionary; contextual embeddings are a conversation. |
