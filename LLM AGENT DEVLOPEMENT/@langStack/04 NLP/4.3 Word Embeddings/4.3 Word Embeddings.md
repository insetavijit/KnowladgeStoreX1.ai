| Topic                                   | Focus & Purpose                                                                                                                                    |
| --------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| **[[4.2.1 Subword Tokenization]]**      | Subword units; advantages; OOV handling; morphology capture; vocabulary efficiency; subword segmentation. Modern tokenization.                     |
| **[[4.2.2 Byte-Pair Encoding (BPE)]]**  | BPE algorithm; merge operations; vocabulary building; GPT tokenization; BPE variants; implementation. Compression-based tokenization.              |
| **[[4.2.3 WordPiece]]**                 | WordPiece algorithm; BERT tokenization; likelihood-based merging; WordPiece vs BPE; Google's approach. BERT-family tokenization.                   |
| **[[4.2.4 SentencePiece]]**             | Language-agnostic tokenization; unsupervised training; unigram LM; SentencePiece models; multilingual; raw text input. Universal tokenization.     |
| **[[4.2.5 Tokenizer Training]]**        | Training custom tokenizers; vocabulary size selection; training corpus; special tokens; tokenizer configuration. Building tokenizers.              |
| **[[4.2.6 Special Tokens]]**            | [CLS], [SEP], [PAD], [UNK], [MASK]; special token handling; positional encoding; token type IDs; attention masks. Model-specific tokens.           |
| **[[4.2.7 Multilingual Tokenization]]** | Cross-lingual tokenization; shared vocabularies; language detection; script handling; Unicode ranges. Global NLP.                                  |
| **[[4.2.8 Tokenization for LLMs]]**     | HuggingFace tokenizers; GPT-4 tokenization; Claude tokenization; token counting; tokenization debugging; fast tokenizers. Production tokenization. |