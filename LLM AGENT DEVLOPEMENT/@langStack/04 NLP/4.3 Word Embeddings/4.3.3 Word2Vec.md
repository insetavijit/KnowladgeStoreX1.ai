| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.3.3 Word2Vec Overview]]**         | Predict context from words (or vice versa)                     | predictive model, shallow neural network, local context window, breakthrough in 2013       | Word2Vec proved that simple prediction tasks yield powerful semantic maps. |
| **[[CBOW (Continuous Bag of Words)]]**  | Predict target word from surrounding context                   | fast training, averaging context vectors, better for frequent words                        | CBOW fills in the blank: "The [cat] sat on the mat".                   |
| **[[Skip-Gram]]**                       | Predict surrounding context from target word                   | slower training, treats each context-target pair effectively, better for rare words        | Skip-gram predicts the neighbors: "[The] [sat] <- cat -> [on] [the]".  |
| **[[Negative Sampling]]**               | Efficient approximation of Softmax                             | binary classification (real vs noise), noise contrastive estimation, speed optimization    | Don't calculate probability for all 100k words; just differentiate from 5 noise words. |
| **[[Hierarchical Softmax]]**            | Alternative training optimization                              | Huffman tree structure, log(V) complexity, better for infrequent words                     | Turning the prediction into a game of 20 questions down a binary tree. |
| **[[Subsampling]]**                     | Handle high-frequency words                                    | dropping "the", "a", "is" probabilistically, improving rare word representations           | Ignoring the noise (common words) to focus on the signal (content words). |
