| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.1.8 Preprocessing Pipelines Overview]]**| Structure the flow of text transformations                     | modularity, sequential composition, dependency management, reproducible workflows          | A pipeline transforms raw chaos into structured features, step by step.|
| **[[Chaining Operations]]**             | Link individual processing steps together                      | input-output compatibility, order of operations (clean -> normalize -> tokenize), piping   | Correct chaining ensures that step N prepares the data correctly for step N+1. |
| **[[spaCy Pipelines]]**                 | Leverage spaCy's built-in processing architecture              | `nlp.pipe()`, disabling components (`disable=['ner']`), custom components, attribute extensions | spaCy offers an industrial-strength, highly distinct pipeline structure. |
| **[[NLTK Pipelines]]**                  | Build custom pipelines using NLTK tools                        | manual chaining, flexibility, educational clarity, integration with pandas apply           | NLTK pipelines are often manual loops but offer granular control.       |
| **[[Custom Pipelines]]**                | Design bespoke workflows for specific tasks                    | error handling, logging, caching intermediate results, parallel processing                 | Custom pipelines are essential when off-the-shelf tools don't fit the data. |
| **[[Reusable Preprocessing]]**          | Save and share pipeline configurations                         | serialization (pickling), configuration files (YAML/JSON), versioning pipelines            | A reusable pipeline ensures training and inference data are treated identically. |
