| **Subtopic**                           | **Focus & Purpose**                                               | **Key Concepts / Details**                                                                  | **One-Line Recall**                                                       |
| -------------------------------------- | ----------------------------------------------------------------- | ------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------- |
| **[[4.1.1 Tokenization overview]]**    | Establish the foundation for text preprocessing and NLP pipelines | core principles, tokenization types, preprocessing workflows, text normalization            | Tokenization transforms raw text into structured, analyzable units.       |
| **[[Word Tokenization]]**              | Split text into individual word-level tokens                      | whitespace splitting, regex patterns, word boundaries, contraction handling, compound words | Word tokenization breaks sentences into discrete lexical units.           |
| **[[Sentence Tokenization]]**          | Segment text into sentence-level boundaries                       | period detection, abbreviation handling, ellipsis, quotation marks, newlines                | Sentence tokenization identifies natural linguistic boundaries in text.   |
| **[[Whitespace Splitting]]**           | Apply basic space-based text segmentation                         | `split()` method, limitations with punctuation, CJK languages, assumptions                  | Whitespace splitting is fast but inadequate for production NLP tasks.     |
| **[[Punctuation Handling]]**           | Manage punctuation marks during tokenization                      | attachment vs. separation, special characters, emoticons, intra-word punctuation            | Strategic punctuation handling preserves semantic meaning during splits.  |
| **[[Tokenization Challenges]]**        | Identify and resolve edge cases in tokenization                   | contractions, URLs, emails, hashtags, numbers with decimals, emoji, multi-word expressions  | Real-world text contains numerous ambiguities requiring careful handling. |
| **[[Language-Specific Tokenization]]** | Adapt tokenization strategies across languages                    | CJK (character-based), agglutinative languages, RTL scripts, morphological complexity       | Effective tokenization requires language-aware algorithms and rules.      |
| **[[Tokenization Tools & Libraries]]** | Leverage existing tokenization implementations                    | NLTK, spaCy, Moses, Stanza, language-specific tokenizers                                    | Production systems benefit from battle-tested tokenization libraries.     |
