| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.1.4 Stop Words Overview]]**       | Manage high-frequency, low-semantic value words                | dimensionality reduction, noise reduction, Zipf's law, sparse matrix optimization          | Stop words are the glue of language but often noise for simple models. |
| **[[Standard Stop Lists]]**             | Utilize pre-defined sets of common words                       | NLTK list, spaCy list, sklearn features, "the", "is", "at", "which", "on"                  | Standard lists provide a quick baseline for filtering common terms.    |
| **[[Custom Stop Words]]**               | Tailor exclusion lists to the specific domain                  | adding domain-specific noise (e.g., "patient" in medical docs), removing essential terms   | Effective filtering often requires manually curating the stop list.    |
| **[[When to Keep Stop Words]]**         | Retain stop words for context-sensitive tasks                  | sentiment analysis (negation), phrase searching, sequence models (BERT/GPT), authorship    | Removing stop words can destroy critical context and nuance.           |
| **[[Language-Specific Lists]]**         | Adapt stopping strategies to different languages               | morphology awareness, grammatical particles, multilingual stop word repositories           | Stop words vary drastically in function and frequency across languages.|
| **[[Impact on Models]]**                | Understand trade-offs of removal vs. retention                 | bag-of-words efficiency vs. deep learning context, precision vs. recall                    | Removal boosts simple models but hinders context-aware deep learning.  |
