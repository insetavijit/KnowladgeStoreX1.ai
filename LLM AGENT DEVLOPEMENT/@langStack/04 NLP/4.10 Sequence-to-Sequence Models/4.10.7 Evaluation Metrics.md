| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.10.7 Evaluation Metrics Overview]]**| Measuring generation quality                                   | N-gram overlap metrics vs Model-based metrics, Human evaluation is paramount               | Judging the essay without reading it.                                  |
| **[[BLEU Score]]**                      | Precision for translation                                      | Bilingual Evaluation Understudy, counting n-gram precision against references, brevity penalty| Checking if the translation contains the right words.                  |
| **[[ROUGE Score]]**                     | Recall for summarization                                       | Recall-Oriented Understudy for Gisting Evaluation, checking overlap with reference abstracts| Checking if the summary included all the key points.                   |
| **[[METEOR]]**                          | Flexible matching                                              | Aligning unigrams considering synonyms, stemming, and reordering, better correlation w/ human| Giving credit for saying "Quick" instead of "Fast".                    |
| **[[BERTScore]]**                       | Semantic similarity                                            | Using BERT embeddings to compare generated text similarity to reference, handling synonyms well| Realizing that "The car is red" and "The automobile is scarlet" are the same.|
| **[[Perplexity]]**                      | Intrinsic measurement                                          | How confident the model is in its own generation, useful for debugging but not quality     | How surprised the model is by the correct answer.                      |
