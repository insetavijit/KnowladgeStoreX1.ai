| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.10.3 Transformer Seq2seq Overview]]**| The Encoder-Decoder Transformer                                | Original "Attention Is All You Need" architecture, T5, BART                                | The architecture that killed RNNs.                                     |
| **[[Full Transformer Architecture]]**   | Encoder Stack + Decoder Stack                                  | Encoder uses Self-Attention; Decoder uses Masked Self-Attention + Cross-Attention          | The complete machine: Reader on the left, Writer on the right.         |
| **[[T5 (Text-to-Text Transfer Transformer)]]**| Unifying tasks                                             | "Translate English to German:", "Summarize:", strict encoder-decoder structure             | Treating every problem (Classification, Translation, QA) as a text generation task.|
| **[[BART (Bidirectional Auto-Regressive)]]**| Denoising autoencoder                                        | BERT-like encoder + GPT-like decoder, excellent for summarization                          | Corrupting the text and asking the model to fix it.                    |
| **[[mT5 / mBART]]**                     | Multilingual variants                                          | Pre-trained on 100+ languages, massive cross-lingual transfer capabilities                 | The Universal Translator.                                              |
| **[[Switch Transformers]]**             | Scaling to trillions                                           | Mixture of Experts (MoE), sparse activation, varying computation per token                 | A massive brain where only small parts light up at a time.             |
