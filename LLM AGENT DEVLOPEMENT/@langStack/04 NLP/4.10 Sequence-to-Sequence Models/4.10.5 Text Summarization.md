| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.10.5 Text Summarization Overview]]**| Compressing information                                        | Reducing length while preserving meaning, headline generation, meeting minutes             | TL;DR.                                                                 |
| **[[Extractive Summarization]]**        | Highlighting key sentences                                     | Ranking sentences by importance (TextRank), selecting top-k, no new words generated        | Using a yellow highlighter to pick the best lines.                     |
| **[[Abstractive Summarization]]**       | Rephrasing and condensing                                      | Seq2Seq models (BART, Pegasus), generating new sentences to synthesize content             | Reading the book and writing a book report in your own words.          |
| **[[Long-Document Summarization]]**     | Handling books/papers                                          | Hierarchical attention, divide-and-conquer, infinite context windows (Gemini 1.5)          | Summarizing "War and Peace" without crashing the GPU.                  |
| **[[Query-Focused Summarization]]**     | Answering specific needs                                       | Summarizing valid content *relevant to a query* ("Summarize his views on taxes")           | Identifying specific needles in the haystack.                          |
| **[[Evaluation (ROUGE)]]**              | Measuring overlap                                              | ROUGE-1, ROUGE-2, ROUGE-L, comparing n-gram overlap with human reference summaries         | checking if the model used the same words as the human summary.        |
