| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.10.2 Attention Mechanisms Overview]]**| Fixing the bottleneck problem                                  | Look at source tokens dynamically, soft alignment, solving the "long distance" issue       | Letting the translator look back at the original sentence while writing.|
| **[[Bahdanau Attention (Additive)]]**   | The original attention                                         | Neural network layer to calculate scores, concatenation-based                              | Computing "How relevant is this input word?" using a tiny neural net.  |
| **[[Luong Attention (Multiplicative)]]**| Dot-product attention                                          | Global vs Local attention, dot product of states, faster and simpler                       | Measuring relevance by simply multiplying the vectors.                 |
| **[[Context Vector Calculation]]**      | The weighted sum                                               | Softmax over scores -> Weights -> Weighted sum of encoder hidden states                    | creating a custom summary vector for *this specific word* I'm writing. |
| **[[Visualization]]**                   | Interpreting the model                                         | Attention heatmaps, alignment matrices, seeing which source word produced which target word| Seeing that the model looked at "Network" when it wrote "RÃ©seau".      |
| **[[Self-Attention vs Cross-Attention]]**| Distinguishing types                                          | Self (Encoder looking at itself), Cross (Decoder looking at Encoder)                       | Self: Understanding the input. Cross: Translating the input.           |
