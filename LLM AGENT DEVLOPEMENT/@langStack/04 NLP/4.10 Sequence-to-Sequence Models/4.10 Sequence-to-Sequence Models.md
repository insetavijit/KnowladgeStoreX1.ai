| Topic                                     | Focus & Purpose                                                                                                |
| ----------------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| **[[4.10.1 Seq2seq Basics]]**             | Encoder-decoder; sequence transduction; RNN seq2seq; architecture overview. Core architecture.                 |
| **[[4.10.2 Attention Mechanisms]]**       | Attention scores; alignment; Bahdanau attention; Luong attention; attention visualization. Focusing mechanism. |
| **[[4.10.3 Transformer Seq2seq]]**        | T5; BART; mT5; encoder-decoder transformers; modern architectures. Transformer generation.                     |
| **[[4.10.4 Machine Translation]]**        | NMT; translation systems; BLEU score; translation evaluation; multilingual models. Language translation.       |
| **[[4.10.5 Text Summarization]]**         | Extractive summarization; abstractive summarization; ROUGE scores; summarization models. Content condensation. |
| **[[4.10.6 Decoding Strategies]]**        | Greedy decoding; beam search; diverse beam search; sampling; length normalization. Generation control.         |
| **[[4.10.7 Evaluation Metrics]]**         | BLEU; ROUGE; METEOR; BERTScore; human evaluation; automatic metrics. Quality measurement.                      |
| **[[4.10.8 Fine-tuning for Generation]]** | Fine-tuning T5/BART; task-specific generation; optimization; generation quality. Adaptation.                   |