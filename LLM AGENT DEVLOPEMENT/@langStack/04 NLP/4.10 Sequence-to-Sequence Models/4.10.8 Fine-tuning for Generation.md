| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.10.8 Fine-tuning for Generation Overview]]**| Adapting foundation models                                     | Instruction tuning, domain adaptation, Parameter-Efficient Fine-Tuning (PEFT)              | Teaching a generalist writer to write legal contracts.                 |
| **[[Instruction Tuning]]**              | Following commands                                             | FLAN, Wei et al., training on (Instruction, Response) pairs to generalize to new tasks     | Teaching the model that "Translate:" means "Don't just continue writing in English".|
| **[[PEFT / LoRA]]**                     | Efficient tuning                                               | Low-Rank Adaptation, freezing main weights and training small adapters, saving GPU memory  | Upgrading the software without replacing the hardware.                 |
| **[[Data Formatting]]**                 | Preparing the dataset                                          | Prompt templates ("Question: ... Answer: ..."), masking the input loss, handling long contexts| Ensuring the student knows which part is the question and which is the answer.|
| **[[Catastrophic Forgetting]]**         | The risk of specialization                                     | Model forgetting general knowledge (English) while learning specific task (Medical French) | Learning math so hard you forget how to speak.                         |
| **[[RLHF (Reinforcement Learning from Human Feedback)]]**| Aligning preferences                                   | PPO, Reward Models, optimizing for "Helpfulness" and "Safety" beyond next-token prediction | Training the dog with treats instead of just reading it a manual.      |
