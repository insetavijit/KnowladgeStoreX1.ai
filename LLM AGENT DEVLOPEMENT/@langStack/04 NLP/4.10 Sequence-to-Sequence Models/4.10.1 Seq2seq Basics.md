| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.10.1 Seq2seq Basics Overview]]**  | Mapping one sequence to another                                | Encoder-Decoder architecture, variable length input/output, end-to-end training            | Translating a Sentence (Length 5) to a Sentence (Length 7).            |
| **[[The Encoder]]**                     | Understanding the input                                        | Processing the source sequence into a fixed-size context vector (hidden state)             | Summarizing the English sentence into a single mathematical thought.   |
| **[[The Decoder]]**                     | Generating the output                                          | Taking the context vector and generating tokens one by one until [EOS]                     | Unpacking the mathematical thought into French.                        |
| **[[Information Bottleneck]]**          | The flaw of vanilla Seq2Seq                                    | Squashing a whole paragraph into one vector causes data loss, forgetting early parts       | Trying to remember an entire lecture in a single keyword.              |
| **[[Teacher Forcing]]**                 | Training strategy                                              | Feeding the *correct* previous token during training, not the model's own guess            | Correcting the student immediately after every word so they don't drift.|
| **[[RNN Implementation]]**              | The classic approach                                           | Sutskever et al. (2014), using LSTMs for both encoder and decoder                          | The OG machine translation model before Attention took over.           |
