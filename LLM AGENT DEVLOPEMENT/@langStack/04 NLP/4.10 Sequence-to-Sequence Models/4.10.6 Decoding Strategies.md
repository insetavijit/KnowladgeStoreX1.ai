| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.10.6 Decoding Strategies Overview]]**| Algorithms for generating text                                 | Search vs Sampling, balancing quality vs diversity, controlling the output                 | The decision process for picking the next word.                        |
| **[[Greedy Search]]**                   | The fastest, dumbest way                                       | Always picking P(max), deterministic, prone to repetition and generic responses            | Taking the first turn you see, hoping it leads home.                   |
| **[[Beam Search]]**                     | Breadth-first exploration                                      | Keeping 'k' best hypotheses, maximizing total sentence probability, standard for MT        | Exploring 5 different paths simultaneously to find the best route.     |
| **[[Top-K Sampling]]**                  | Truncated random sampling                                      | Sampling from the top K most likely words, cutting off the long tail of nonsense           | Rolling a die, but only for the 50 most likely words.                  |
| **[[Nucleus (Top-p) Sampling]]**        | Dynamic truncation                                             | Sampling from the smallest set of words whose cumulative probability > p (e.g., 0.9)       | Adjusting the menu size based on how obvious the next word is.         |
| **[[Contrastive Decoding]]**            | Promoting differences                                          | Subtracting probabilities of a smaller model from a larger model to remove generic answers | Forcing the expert model to say something the amateur wouldn't.        |
