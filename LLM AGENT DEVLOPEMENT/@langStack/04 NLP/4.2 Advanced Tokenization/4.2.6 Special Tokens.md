| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.2.6 Special Tokens Overview]]**   | Mark structural and functional boundaries in text              | sequence start/end, padding, masking, separators, model-specific semantics                 | Special tokens shape how the model perceives the structure of the input. |
| **[[[CLS] and [SEP]]]**                 | Define sentence and input boundaries (BERT-style)              | `[CLS]` for classification embedding, `[SEP]` to delimit segments in pairs                 | `[CLS]` summarizes the sentence; `[SEP]` tells where it ends.          |
| **[[[PAD] Token]]**                     | Standardize batch sequence lengths                             | padding to max length, attention masking (ignoring pads), zero-vector representation       | `[PAD]` is the filler that makes variable-length text fit into fixed-size tensors. |
| **[[[UNK] Token]]**                     | Handle unknown words (Out-of-Vocabulary)                       | fallback for characters/words not in vocab, minimizing its occurrence via BPE/WordPiece    | `[UNK]` is the tokenizer's way of saying "I have no idea what this is." |
| **[[[MASK] Token]]**                    | Facilitate Masked Language Modeling (MLM)                      | hiding tokens for prediction, pre-training objective, corruption strategy                  | `[MASK]` is the blank that the model learns to fill in.                |
| **[[Model-Specific Tokens]]**           | Custom tokens for specific architectures                       | `<s>`, `</s>` (RoBERTa/BART), `<|endoftext|>` (GPT), `[INST]` (Llama instruction tuning)   | Every model architecture has its own dialect of control symbols.       |
