| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.2.1 Subword Tokenization Overview]]**| Balance vocabulary size and unknown word handling              | OOV (Out-of-Vocabulary) problem, hybrid approach, morphology, open vocabulary assumption   | Subwords break "unfriend" into "un" + "friend" to understand new words.|
| **[[Subword Units]]**                   | Define the fundamental building blocks of modern NLP           | characters, valid subwords, whole words, splitting strategies, token limits                | Units are smaller than words but larger than characters.               |
| **[[Advantages]]**                      | Why subwords became the standard for LLMs                      | smaller vocab size (efficiency), no UNK tokens (coverage), sharing meaning via root sharing | Subwords solve the massive vocabulary problem of word-based models.    |
| **[[OOV Handling]]**                    | Manage words not seen during training                          | fallback to characters, composing new words from known sub-parts, infinite vocabulary effect| If you can write it, subword tokenizers can tokeninize it.             |
| **[[Morphology Capture]]**              | Encapsulate linguistic structure implicitly                    | prefixes, suffixes, root stems, grammatical markers, agglutinative language handling       | "Playing" -> "play" + "##ing" captures the verb and the tense.         |
| **[[Vocabulary Efficiency]]**           | Optimize the trade-off between sequence length and vocab size  | character-level (long sequences) vs word-level (huge vocab), sweet spot optimization       | Subwords offer the "Goldilocks" zone of tokenization.                  |
