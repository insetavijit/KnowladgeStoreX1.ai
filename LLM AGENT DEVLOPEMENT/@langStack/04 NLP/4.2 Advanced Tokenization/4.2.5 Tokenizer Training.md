| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.2.5 Tokenizer Training Overview]]**| Learn a vocabulary from a specific corpus                      | domain adaptation, statistical learning, optimization of token utility, maximizing coverage| Training a tokenizer means "teaching" it the language it will encode.  |
| **[[Training Corpus]]**                 | Select the text data to learn from                             | representativeness, size requirements, cleaning before training, domain specificity        | Your tokenizer is only as good as the data it was trained on.          |
| **[[Vocabulary Size]]**                 | Choose the optimal number of tokens                            | trade-off: model size vs sequence length, diminishing returns, standard sizes (30k-100k)   | Too small = long sequences; Too large = sparse embedding matrix.       |
| **[[Special Tokens]]**                  | Reserve slots for structural markers                           | `[PAD]`, `[UNK]`, `[CLS]`, `[SEP]`, `[MASK]`, adding tokens after training vs during       | You must define your control symbols before you start training.        |
| **[[Configuration]]**                   | Set hyperparameters for the tokenizer                          | algorithm choice (BPE/WordPiece), limit_alphabet, min_frequency, normalization rules       | Configuration defines the rules of the game for the training algorithm.|
| **[[Saving & Loading]]**                | Persist the trained model for reuse                            | `vocab.txt` (BERT), `tokenizer.json` (HuggingFace), compatibility with model checkpoints   | A trained tokenizer is an essential artifact alongside the model weights.|
