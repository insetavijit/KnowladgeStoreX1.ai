| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.2.3 WordPiece Overview]]**        | Maximize likelihood of training data                           | likelihood-based merging, language independent, BERT's tokenizer, subword regularization   | WordPiece optimizes for the probability of the text, not just frequency. |
| **[[WordPiece Algorithm]]**             | How WordPiece selects merges                                   | score = freq(pair) / (freq(first) * freq(second)), maximizing language model likelihood    | WordPiece merges tokens that are surprisingly frequent together.       |
| **[[WordPiece vs BPE]]**                | Comparison of the two dominant algorithms                      | frequency (BPE) vs likelihood (WordPiece), handling of prefixes/suffixes, performance      | BPE counts occurrences; WordPiece measures statistical association.    |
| **[[BERT Tokenization]]**               | Specifics of how BERT uses WordPiece                           | `##` prefix for suffixes, lowercasing, accent stripping, [UNK] token handling              | BERT's `##ing` format is the signature of WordPiece tokenization.      |
| **[[Google's Approach]]**               | Origins and evolution of WordPiece                             | Japanese/Korean voice search origin, NMT (Neural Machine Translation) adoption             | Google built WordPiece to solve Asian language tokenization challenges.|
| **[[Unigram Language Model]]**          | Theoretical foundation often linked to WordPiece               | probability of subword sequences, consistency of segmentation, relation to SentencePiece   | WordPiece selects the vocab that makes the training data most probable.|
