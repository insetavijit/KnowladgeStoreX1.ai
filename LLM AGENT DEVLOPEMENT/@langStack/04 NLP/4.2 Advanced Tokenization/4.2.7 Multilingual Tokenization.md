| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.2.7 Multilingual Tokenization Overview]]**| Process text from dozens or 100+ languages simultaneously      | unified vocabulary, language neutrality, zero-shot transfer, curse of multilinguality      | Multilingual tokenization finds a common ground between diverse languages. |
| **[[Shared Vocabularies]]**             | Use one vocabulary for all languages                           | token overlap (e.g., numbers, common names), reducing total param count, alignment         | A shared vocab allows a model to transfer knowledge from English to Swahili. |
| **[[Script Handling]]**                 | Manage different writing systems                               | Latin, Cyrillic, CJK, Arabic, Devanagari, character coverage, uni-code normalization       | You can't tokenize what you can't represent; script support is foundational. |
| **[[Language Detection]]**              | (Optional) Identify language for specific preprocessing        | heuristics, model-based detection (fastText), routing to specific tokenizers (if distinct) | Sometimes you need to know it's French to tokenize "l'homme" correctly. |
| **[[Byte-Level Fallback]]**             | Handle unseen characters via UTF-8 bytes                       | fallback to raw bytes for rare scripts, ensuring no data loss, strictly matching BPE/BBPE  | If a character isn't in the vocab, its constituent bytes definitely are. |
| **[[Sampling Strategies]]**             | Balance high-resource vs low-resource languages                | upsampling low-resource data, alpha-smoothing, preventing English dominance                | Without rebalancing, the tokenizer effectively becomes an English tokenizer. |
