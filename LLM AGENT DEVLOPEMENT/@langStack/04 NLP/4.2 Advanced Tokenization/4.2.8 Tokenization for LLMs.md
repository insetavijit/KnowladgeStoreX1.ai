| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.2.8 Tokenization for LLMs Overview]]**| Apply modern tokenization to massive language models           | Tiktoken, efficiency, cost estimation, sliding windows, context window management          | In the LLM era, tokenization equates directly to compute cost and context capacity. |
| **[[HuggingFace Tokenizers]]**          | The de facto standard library for NLP tokenization             | Rust backend (speed), Python bindings, pre-trained tokenizer hub, ease of use              | HuggingFace provides the standardized interface for almost all modern tokenizers.   |
| **[[GPT-4 / Claude Tokenization]]**     | Specifics of proprietary model tokenizers                      | `cl100k_base` (OpenAI), different whitespace handling, improved coding capability          | Newest tokenizers are optimized for code and multilingual text, distinct from GPT-2. |
| **[[Token Counting]]**                  | Calculate usage and costs accurately                           | `len(enc.encode(text))`, pricing per 1k tokens, prompt vs completion token counts          | Accurate counting is the first step in cost control and context management.         |
| **[[Fast Tokenizers]]**                 | Optimize for production throughput                             | Rust-based implementations, parallel processing, offset mapping (aligning tokens to text)  | When processing terabytes, Python loops are too slow; use the Rust backend.         |
| **[[Tokenization Debugging]]**          | Inspect and fix tokenization issues                            | `decode()`, visualizers, checking for weird splits (e.g., "Hello" -> "He", "llo")         | If the model acts weird, check if the tokenizer is chopping words unexpectedly.     |
