| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.2.4 SentencePiece Overview]]**    | Provide a language-independent tokenizer                       | raw text input, no pre-tokenization required, reversible tokenization                      | SentencePiece treats text as a stream of raw unicode characters.       |
| **[[Language-Agnostic]]**               | Eliminate language-specific logic                              | removal of whitespace dependency, handling valid unicode sequences instead of words        | It doesn't care if it's English, Japanese, or binary code.             |
| **[[Unsupervised Training]]**           | Learn vocabulary directly from raw sentences                   | Expectation-Maximization (EM), optimizing likelihood, BPE or Unigram modes                 | It learns where the "words" are purely from statistical patterns.      |
| **[[Unigram Language Model]]**          | The default algorithm in SentencePiece                         | probabilistic segmentation, sub-optimal split pruning, sampling for regularization         | Unigram starts big and prunes down; BPE starts small and merges up.    |
| **[[Raw Text Input]]**                  | Process text without prior whitespace splitting                | `_` (underscore) as space, treating space as a normal character, lossless reconstruction   | SentencePiece sees "Hello world" as "Hello_world", preserving exact spaces. |
| **[[Multilingual Models]]**             | Facilitate unified models like T5 and ALBERT                   | shared vocabulary across 100+ languages, handling script mixing naturally                  | It's the standard for modern multilingual models like mBERT and XLM-R. |
