| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.2.2 BPE Overview]]**              | Iteratively merge most frequent character pairs                | compression algorithm, bottom-up merging, frequency-based construction, pre-tokenization   | BPE builds a vocabulary by gluing the most common neighbors together.  |
| **[[BPE Algorithm]]**                   | The core mechanics of training a BPE tokenizer                 | 1) Init vocab with chars 2) Count pairs 3) Merge best pair 4) Repeat until size limit      | BPE is a simple, greedy iterative process of pair merging.             |
| **[[Vocabulary Building]]**             | Determine the final set of tokens                              | base characters + merged pairs, stopping criteria, controlling model size                  | The desired vocab size dictates when the merging stops.                |
| **[[GPT Tokenization]]**                | How GPT-2/3/4 use BPE                                          | byte-level BPE, handling Unicode via bytes, 256 base tokens, avoiding UNK entirely         | GPT's BPE works on bytes, not characters, ensuring 100% coverage.      |
| **[[BPE Variants]]**                    | Modifications to the standard algorithm                        | Byte-level BPE (BBPE), BPE-Dropout (regularization), limiting merge distance               | Variants adapt BPE for better coverage or model robustness.            |
| **[[Implementation]]**                  | Practical application of BPE                                   | `tokenizers` library, `huggingface/tokenizers`, encoding/decoding rules, offsets           | Efficient BPE implementation is standard in all modern NLP libraries.  |
