| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.5.4 Neural NER Overview]]**       | Use deep learning to capture complex entity patterns           | word embeddings, architectural shifts, end-to-end learning, automated feature generation   | Deep learning replaced hand-crafted features with learned dense vectors. |
| **[[BiLSTM-CRF]]**                      | The classic state-of-the-art pre-BERT architecture             | Bi-directional LSTM (context), CRF layer (valid transitions), character embeddings (morphology)| BiLSTM looks both ways; CRF ensures the path makes sense.              |
| **[[BERT for NER]]**                    | The modern standard for high performance                       | Token classification head, fine-tuning pre-trained transformers, subword handling          | BERT puts a simple classifier on top of its massive brain to solve NER.|
| **[[Token Classification Head]]**       | The specific layer added to transformers                       | Linear layer on top of hidden states, softmax over tag set (O, B-PER, I-PER, etc.)         | A simple linear map from "768-dim vector" to "Person or not?".         |
| **[[Contextual Embeddings]]**           | Why they changed the game for NER                              | separating "Bank of America" (Org) from "bank of the river" (Loc) via context              | Contextual vectors solve disambiguation, the hardest part of NER.      |
| **[[Character Embeddings]]**            | Handling morphology and finding patterns in spelling           | Conv1D / LSTM over characters, combined with word embeddings, robustness to typos          | Seeing that "Sulfamethoxazole" looks like a drug name just by its letters.|
