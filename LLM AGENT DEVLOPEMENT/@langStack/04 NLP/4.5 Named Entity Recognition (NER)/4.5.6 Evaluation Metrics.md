| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.5.6 Evaluation Metrics Overview]]**| Correctly measure NER performance                              | precision, recall, F1-score, span-level vs token-level, strict vs relaxed                  | Accuracy is a lie in NER; use F1 on entities, not tokens.              |
| **[[Span-Level Evaluation]]**           | The gold standard for NER                                      | exact match of boundary and type, "New York" must use "New" and "York" as one unit         | Getting half the name right counts as a zero in strict evaluation.     |
| **[[Token-Level Evaluation]]**          | A naive and often misleading metric                            | accuracy per word, O-class dominance problem (90% of tokens are O)                         | 99% accuracy is easy if you just predict "Not an Entity" for everything.|
| **[[Precision vs Recall]]**             | Balancing false positives and false negatives                  | Precision (trustworthiness of findings), Recall (coverage of truths), trade-offs           | Precision means "what I found is real"; Recall means "I found everything real".|
| **[[F1 Score]]**                        | The harmonic mean of precision and recall                      | balanced metric, standard benchmark score, CoNLL-2003 evaluation script                    | The single number that tells you if your model is actually working.    |
| **[[Partial Matching]]**                | A softer metric for practical use                              | counting overlap, type correctness with boundary error, "fuzzy" evaluation                 | Sometimes finding "University of Texas" when the label was just "Texas" is okay.|
