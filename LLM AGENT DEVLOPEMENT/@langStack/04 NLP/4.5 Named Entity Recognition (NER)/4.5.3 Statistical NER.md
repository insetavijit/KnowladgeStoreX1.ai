| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.5.3 Statistical NER Overview]]**  | Learn entity patterns from probabilistic features              | probability distributions, feature engineering, sequence modeling, pre-deep learning       | Math that learns "Capitalized word after 'Mr.' is usually a Person."   |
| **[[CRF (Conditional Random Fields)]]** | The dominant pre-neural sequence model                         | global sequence probability, transition features, state features, overcoming HMM limits    | CRF looks at the whole sentence to decide if "Bank" is a river bank.   |
| **[[Sequence Labeling]]**               | The framing of NER as a token classification task              | input sequence -> output sequence of tags, structured prediction                           | Assigning a label (B-PER, I-PER, O) to every single token along the line.|
| **[[Feature Engineering]]**             | Manually creating clues for the model                          | prefix/suffix measures, window context, POS tags, capitalization flags, word shapes        | Telling the model: "Look at the last 3 letters" and "Is it all caps?". |
| **[[MaxEnt (Maximum Entropy)]]**        | Logistic regression for sequences                              | multi-class classification, feature weights, independence assumptions                      | Making the least biased decision consistent with the observed data.    |
| **[[Transition Features]]**             | Learning what labels follow what labels                        | probability of I-PER following B-PER, probability of O following I-LOC                     | Learning that an "Inside Person" tag never follows an "Inside Location" tag.|
