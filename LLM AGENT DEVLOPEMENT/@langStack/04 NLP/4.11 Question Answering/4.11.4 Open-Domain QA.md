| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.11.4 Open-Domain QA Overview]]**  | Answering arbitrary questions                                  | Answering from Wikipedia/Web without a pre-defined passage, large-scale information retrieval| QA on the entire internet.                                             |
| **[[Retriever-Reader Pipeline]]**       | The standard recipe                                            | Step 1: Find 5 relevant docs (Retriever). Step 2: Extract answer from them (Reader).       | Google search + Reading the top result.                                |
| **[[DPR (Dense Passage Retrieval)]]**   | Semantic searching                                             | Training a bi-encoder (Question Enc, Document Enc) to map relevant pairs close in vector space| Finding key documents by meaning, not just keyword matching.           |
| **[[ORQA (Open-Retrieval QA)]]**        | End-to-end training                                            | Pre-training the retriever *for* the QA task, creating an Inverse Cloze Task (ICT)         | Training the librarian specifically to help with the quiz.             |
| **[[REALM / RAG]]**                     | Retrieval-Augmented approaches                                 | Conditioning the LM generation on retrieved documents, differentiable retrieval            | Giving the model an open-book test.                                    |
| **[[Closed-Book QA]]**                  | No retrieval                                                   | Using a massive LM (like GPT-3) to answer solely from parametric memory (weights)          | Reciting facts from memory without looking them up.                    |
