| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.7.5 Transformer Classifiers Overview]]**| State-of-the-art text classification                           | BERT, RoBERTa, DistilBERT, scaling to long sequences, [CLS] token usage                    | Using a pre-trained genius to stick a label on the document.           |
| **[[Sequence Classification Head]]**    | The output layer                                               | [CLS] token -> Dense Layer -> Softmax, adding a simple classifier on top of a giant brain  | The tiny hat on top of the giant robot that does the actual sorting.   |
| **[[Fine-Tuning]]**                     | Adapting the pre-trained model                                 | updating all weights vs freezing layers, catastrophic forgetting, learning rate scheduling | Teaching a college professor how to sort mail.                         |
| **[[Long Document Classification]]**    | Handling text > 512 tokens                                     | Longformer, BigBird, sliding window + voting, truncation strategies                        | Reading the whole book instead of just the first page.                 |
| **[[Multi-Label w/ Transformers]]**     | Thresholding outputs                                           | Sigmoid instead of Softmax, independent probabilities per class, binary cross-entropy      | When the document is both "Urgent" AND "Finance".                      |
| **[[Efficiency Distillation]]**         | Making it faster                                               | DistilBERT, MobileBERT, compressing the giant model for production                         | Shrinking the genius into a size that fits on your phone.              |
