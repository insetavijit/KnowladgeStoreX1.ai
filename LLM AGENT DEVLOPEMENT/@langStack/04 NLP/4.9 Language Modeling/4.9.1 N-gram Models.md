| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.9.1 N-gram Models Overview]]**    | Predicting the next word based on previous N-1 words           | Markov assumption, Unigrams (1), Bigrams (2), Trigrams (3), joint probability              | Guessing "York" because you just saw "New".                            |
| **[[Smoothing Techniques]]**            | Handling zero probabilities                                    | Laplace (Add-1), Add-k, Good-Turing, preventing the model from saying "Impossible" for unseen words| Assigning a tiny probability to "pig flying" just in case.             |
| **[[Backoff & Interpolation]]**         | Falling back to simpler models                                 | Katz Backoff, Kneser-Ney smoothing, using bigrams when trigrams aren't found               | If you don't know the 3-word phrase, try looking for the 2-word phrase.|
| **[[Language Model Probabilities]]**    | Scoring sentences                                              | Calculating P(Sentence) as product of P(word|history), chain rule                      | Calculating how likely "The cat sat on the mat" is vs "The mat sat on the cat".|
| **[[Limitations of N-grams]]**          | Why we stopped using them                                      | Sparsity (curse of dimensionality), lack of long-range context, inability to generalize    | Failing to know "bank" means "river" because the word "river" was 10 words ago.|
| **[[Vocabulary Problems]]**             | Dealing with OOV                                               | Closed vocabulary vs Open vocabulary, [UNK] token, handling unknown words                  | What do you do when you see a word you've never learned before?        |
