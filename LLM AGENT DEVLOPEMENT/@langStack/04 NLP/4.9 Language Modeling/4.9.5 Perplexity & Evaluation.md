| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.9.5 Perplexity & Evaluation Overview]]**| Measuring how "surprised" a model is                           | Intrinsic evaluation, lower is better, exponentiated cross-entropy                         | A score of how confused the model is by real human text.               |
| **[[Perplexity (PPL)]]**                | The standard metric                                            | PPL = 2^(-log probability), equivalent to the branching factor (weighted average choices)  | If PPL is 10, the model is as confused as rolling a 10-sided die.      |
| **[[Cross-Entropy Loss]]**              | The training objective                                         | Minimizing the negative log-likelihood of the true next token                              | Penalizing the model heavily for being confident and wrong.            |
| **[[Bits-Per-Character (BPC)]]**        | Character-level metric                                         | Used for character LMs, independent of vocabulary size, information theory basis           | How many bits it takes to compress the text using the model.           |
| **[[Downstream Evaluation]]**           | Extrinsic evaluation                                           | GLUE, SuperGLUE, MMLU, judging the LM by how well it solves actual tasks                   | Don't ask if the student knows the textbook; ask if they can pass the exam.|
| **[[Human Evaluation]]**                | Quality assessment                                             | Fluency, Coherence, Factuality, ELO ratings (Chatbot Arena)                                | Asking people "Which response do you like better?"                     |
