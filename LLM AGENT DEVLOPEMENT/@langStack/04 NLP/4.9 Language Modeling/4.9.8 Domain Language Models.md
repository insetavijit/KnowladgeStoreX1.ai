| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.9.8 Domain Language Models Overview]]**| Specializing LMs for verticals                                 | Adapting to shifts in vocabulary and syntax (Medical, Legal, Finance, Code)                | Sending the English major to Law School.                               |
| **[[BioBERT / ClinicalBERT]]**          | Medical NLP                                                    | Pre-training on PubMed/MIMIC-III, handling complex medical terminology                     | A model that knows "Myocardial Infarction" means "Heart Attack".       |
| **[[LegalBERT]]**                       | Legal NLP                                                      | Trained on case law/contracts, understanding "heretofore", "plaintiff", and long sentences | A model that writes like a lawyer (unfortunately).                     |
| **[[FinBERT]]**                         | Financial NLP                                                  | Sentiment analysis on financial news, understanding that "bullish" is good, not an animal  | Knowing that "volatility" is scary for investors.                      |
| **[[Code LMs]]**                        | Programming languages                                          | Codex, StarCoder, pre-training on GitHub, predicting syntax structure                      | A model that speaks Python, Java, and C++.                             |
| **[[Domain Adaptation Strategies]]**    | How to build them                                              | Continued pre-training (Don't start from scratch), vocabulary expansion, adapters          | Taking a generic genius and making them read 10,000 medical textbooks. |
