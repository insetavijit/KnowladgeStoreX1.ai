| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.9.3 Transformer Language Models Overview]]**| The modern standard (GPT style)                                | Self-attention, non-sequential processing, positional encodings, massive parallelization   | The architecture behind ChatGPT.                                       |
| **[[Causal Masking]]**                  | Preventing peeking ahead                                       | Masking future tokens in self-attention, ensuring autoregressive property                  | Covering the right side of the page so you can only read what you've written so far.|
| **[[GPT Architecture]]**                | Decoder-only Transformers                                      | Stacked decoder blocks, pre-training on large corpus, predicting next token                | A giant machine designed to answer "What comes next?"                  |
| **[[Scaling Laws]]**                    | Bigger is better                                               | Kaplan et al., relationship between parameters, data, compute, and loss                    | More data + More compute + Bigger model = Smarter AI.                  |
| **[[Context Window]]**                  | How much it can "see"                                          | Limited attention span (4k, 8k, 128k tokens), quadratic complexity of attention            | The amount of text the model can keep in its working memory at once.   |
| **[[Positional Encoding]]**             | Injecting order                                                | Sinusoidal encodings, learned embeddings, Rotary Embeddings (RoPE), ALiBi                  | Telling the model that "Dog bites man" is different from "Man bites dog".|
