| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.9.4 Masked Language Models Overview]]**| Learning context from both directions                          | BERT (Bidirectional Encoder Representations from Transformers), Masked Token Prediction    | Filling in the blanks: "The *[MASK]* sat on the mat."                  |
| **[[The Masking Objective]]**           | Training strategy                                              | Randomly masking 15% of tokens (80% [MASK], 10% Random, 10% Original), predicting original | Forcing the model to look left AND right to guess the middle word.     |
| **[[Bidirectionality]]**                | Why it's not generative                                        | Seeing the future tokens prevents auto-regressive generation, strictly for understanding   | You can't write a story if you already know the ending (and the ending helps you write the beginning).|
| **[[RoBERTa & DeBERTa]]**               | Improvements on BERT                                           | Dynamic masking, removing Next Sentence Prediction (NSP), larger batches, disentangled attention| BERT but trained longer, harder, and smarter.                          |
| **[[Span Masking]]**                    | Masking chunks                                                 | SpanBERT, masking contiguous sequences instead of random words, better for coreference     | Guessing "New York City" instead of just "York".                       |
| **[[ELECTRA]]**                         | Efficient training                                             | Replaced Token Detection (Discriminator) instead of Generator, sample efficiency           | Spotting the fake word instead of guessing the missing word.           |
