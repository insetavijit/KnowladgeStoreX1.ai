| Topic                                     | Focus & Purpose                                                                                             |
| ----------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| **[[4.9.1 N-gram Models]]**               | Unigrams, bigrams, trigrams; smoothing; backoff; interpolation; n-gram probability. Classical LMs.          |
| **[[4.9.2 Neural Language Models]]**      | RNN LMs; LSTM LMs; perplexity; training LMs; next token prediction. Neural LMs.                             |
| **[[4.9.3 Transformer Language Models]]** | GPT architecture; causal attention; autoregressive LMs; decoder-only; scaling laws. Modern LMs.             |
| **[[4.9.4 Masked Language Models]]**      | BERT pre-training; masked token prediction; bidirectional context; MLM objective. Encoder LMs.              |
| **[[4.9.5 Perplexity & Evaluation]]**     | Perplexity metric; evaluation sets; cross-entropy; bits-per-character; model comparison. Measuring quality. |
| **[[4.9.6 Text Generation]]**             | Sampling strategies; temperature; top-k; top-p (nucleus); beam search; generation quality. Generating text. |
| **[[4.9.7 Conditional Generation]]**      | Seq2seq; encoder-decoder; conditional LMs; controlled generation; style transfer. Task-specific generation. |
| **[[4.9.8 Domain Language Models]]**      | Domain adaptation; specialized LMs; medical LMs; code LMs; legal LMs. Vertical LMs.                         |