| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.9.2 Neural Language Models Overview]]**| Using neural nets to predict words                             | Word embeddings as input, hidden layers for context, Softmax output, generalizing semantics | A brain that guesses the next word instead of just counting.           |
| **[[Feed-Forward LMs]]**                | Fixed window neural models                                     | Bengio et al. (2003), concatenating fixed previous N words, similar limitation to N-grams  | A neural network that still only looks at the last 3 words.            |
| **[[RNN LMs]]**                         | Recurrent Neural Networks                                      | Hidden state passing, handling arbitrary length history, Backpropagation Through Time (BTT)| Reading the sentence word by word and keeping a running memory.        |
| **[[LSTM / GRU LMs]]**                  | Solving vanishing gradients                                    | Gating mechanisms (Forget, Input, Output), capturing longer dependencies than vanilla RNNs | Remembering the "subject" from the start of the paragraph.             |
| **[[Limitations of RNNs]]**             | Why we moved to Transformers                                   | Sequential computation (slow training), difficulty retaining very long context             | You can't parallelize reading a book; you have to read page 1 before page 2.|
| **[[Character-Level LMs]]**             | Modeling text char by char                                     | Predicting 'h'-'e'-'l'-'l'-'o', no OOV problem, smaller vocabulary                         | Typing one letter at a time instead of whole words.                    |
