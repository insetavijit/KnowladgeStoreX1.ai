| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.4.4 Sentence Embeddings Overview]]**| Encode entire sentences into meaningful vectors                | semantic similarity, clustering, retrieval, dense vector indexes                           | A single vector that captures the "vibe" of the whole sentence.        |
| **[[SBERT (Sentence-BERT)]]**           | The gold standard for sentence embeddings                      | siamese networks, triplet loss, fine-tuning BERT for cosine similarity                     | Retraining BERT so that "cosine similarity" actually means "semantic similarity".|
| **[[Pooling Methods]]**                 | How to condense token vectors                                  | Mean Pooling (average of all), Max Pooling, [CLS] (often poor for similarity), attention   | Averaging the word vectors is usually the best simple way to get a sentence vector.|
| **[[Semantic Similarity]]**             | Measuring how close two sentences are                          | cosine similarity, dot product, STS (Semantic Textual Similarity) benchmarks               | "I love dogs" should be close to "Canines are great", not "I hate dogs".|
| **[[Cross-Encoders vs Bi-Encoders]]**    | Trade-off between accuracy and speed                           | Bi-Encoder (fast retrieval, separate vectors), Cross-Encoder (slow, high accuracy, pairs)  | Bi-Encoders for search (retrieval); Cross-Encoders for sorting (reranking).|
| **[[Universal Sentence Encoder]]**      | Google's alternative to SBERT                                  | transformer-based or DAN (Deep Averaging Network), optimized for transfer learning         | A robust, general-purpose vectorizer for when you just need things to work.|
