| Topic                                  | Focus & Purpose                                                                                                                                |
| -------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| **[[4.4.1 Transformer Architecture]]** | Self-attention; multi-head attention; feedforward layers; layer normalization; residual connections; architecture overview. Core architecture. |
| **[[4.4.2 BERT Embeddings]]**          | Bidirectional encoding; [CLS] token; pooling strategies; layer selection; BERT variants (RoBERTa, ALBERT); extraction. Encoder embeddings.     |
| **[[4.4.3 GPT Embeddings]]**           | Autoregressive encoding; causal attention; GPT-2, GPT-3; decoder-only architecture; generation vs understanding. Decoder embeddings.           |
| **[[4.4.4 Sentence Embeddings]]**      | Sentence-BERT; pooling methods; mean pooling; CLS pooling; semantic similarity; sentence transformers. Text-level representations.             |
| **[[4.4.5 Cross-lingual Embeddings]]** | Multilingual BERT; XLM-RoBERTa; mBERT; zero-shot cross-lingual; alignment; translation-free NLP. Global representations.                       |
| **[[4.4.6 Domain-Specific Models]]**   | BioBERT; SciBERT; FinBERT; LegalBERT; domain adaptation; specialized vocabularies. Vertical NLP.                                               |
| **[[4.4.7 Embedding Extraction]]**     | HuggingFace transformers; layer selection; aggregation strategies; batch processing; efficient extraction. Practical usage.                    |
| **[[4.4.8 Fine-tuning Strategies]]**   | Fine-tuning vs feature extraction; learning rates; freezing layers; adapter layers; LoRA; efficient fine-tuning. Adaptation techniques.        |