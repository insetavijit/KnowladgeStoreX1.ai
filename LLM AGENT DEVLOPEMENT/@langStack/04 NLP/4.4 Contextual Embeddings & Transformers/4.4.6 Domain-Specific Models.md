| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.4.6 Domain-Specific Models Overview]]**| Adapt general models to specialized jargon                     | domain shift, vocabulary mismatch, performance gap, specialized pre-training               | General BERT thinks "viral" means "popular"; BioBERT knows it means "disease".|
| **[[BioBERT / PubMedBERT]]**            | Biomedical NLP                                                 | trained on PubMed/PMC, handling medical terminology, chemical entities, clinical notes     | Specialized for the complex language of medicine and biology.          |
| **[[SciBERT]]**                         | Scientific literature                                          | trained on Semantic Scholar (CS + Bio), handling formulas and citations                    | Understanding the academic dialect of research papers.                 |
| **[[FinBERT]]**                         | Financial sentiment and analysis                               | trained on financial news/filings, sentiment (bullish/bearish), market-specific nuance     | Knowing that "liability" isn't just a generic bad thing, it's a balance sheet item.|
| **[[LegalBERT]]**                       | Law and contracts                                              | trained on legislation/case law, handling legalese, long sentences, specific formulation   | Decoding the dense, archaic language of the courtroom.                 |
| **[[Domain Adaptation]]**               | Strategies to enable specialization                            | continued pre-training (DAPT), task-adaptive pre-training (TAPT), vocabulary augmentation  | Taking a general graduate (BERT) and giving them a pH.D. in a specific field.|
