| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.4.2 BERT Embeddings Overview]]**  | Bidirectional Encoder Representations from Transformers        | encoder-only architecture, deep bi-directionality, pre-training objectives (MLM, NSP)      | BERT looks left and right simultaneously to understand deep context.   |
| **[[[CLS] Token]]**                     | Aggregate sentence-level representation                        | classification token, pooler output, first token embedding, downstream usage               | The single vector that summarizes the entire input sequence.           |
| **[[Layer Selection]]**                 | Choosing which layer's output to use                           | last hidden state, concat last 4 layers, sum last 4 layers, layer 1 vs layer 12 features   | Different layers capture different things (lower=syntax, higher=semantics).|
| **[[Pooling Strategies]]**              | Combining token vectors into a sentence vector                 | max pooling, mean pooling, CLS token, attention-weighted pooling                           | How to squish a sequence of 10 vectors into 1 vector.                  |
| **[[BERT Variants]]**                   | Optimized versions of the original                             | RoBERTa (robust), DistilBERT (fast), ALBERT (lite), DeBERTa (disentangled attention)       | The BERT family tree: faster, smaller, or smarter versions of the original.|
| **[[Feature Extraction]]**              | Using BERT as a frozen embedding generator                     | fixed weights, input to classifiers, semantic search, replacing Word2Vec                   | Using BERT to generate super-powered vectors without training it.      |
