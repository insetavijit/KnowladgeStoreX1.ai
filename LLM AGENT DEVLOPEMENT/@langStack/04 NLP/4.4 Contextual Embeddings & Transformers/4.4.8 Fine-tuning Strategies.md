| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.4.8 Fine-tuning Strategies Overview]]**| Adapt pre-trained models to specific tasks                     | transfer learning, catastrophic forgetting, inductive transfer, downstream performance     | Standing on the shoulders of giants (and teaching them a new trick).   |
| **[[Full Fine-tuning]]**                | Updating all model parameters                                  | high resource cost, best potential performance, risk of forgetting, needs standard LR      | Retraining the whole brain for a new job.                              |
| **[[Feature Extraction (Freezing)]]**   | Updating only the final classification head                    | low cost, faster training, preserving pre-trained knowledge, safe for small data           | Keeping the brain frozen and just teaching the hands what to do.       |
| **[[Discriminative Learning Rates]]**   | Different learning rates for different layers                  | lower LR for early layers (foundational), higher LR for top layers (task-specific)         | Don't mess with the foundations as much as you mess with the roof.     |
| **[[Adapter Modules]]**                 | Inserting small trainable layers                               | parameter efficiency, bottleneck layers, freezing backbone, modularity                     | Injecting tiny trainable chips into the frozen model.                  |
| **[[LoRA (Low-Rank Adaptation)]]**      | Efficient fine-tuning via rank decomposition                   | A and B matrices, updating weights via low-rank delta, indistinguishable from full finetuning outcome | The state-of-the-art way to fine-tune massive models on consumer hardware.|
