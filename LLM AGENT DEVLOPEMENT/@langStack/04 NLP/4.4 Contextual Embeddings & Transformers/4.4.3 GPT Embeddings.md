| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.4.3 GPT Embeddings Overview]]**   | Generative Pre-trained Transformer                             | decoder-only architecture, causal attention (masked), left-to-right processing, generation | GPT predicts the future; it only looks back at what happened.          |
| **[[Causal Attention]]**                | Prevent seeing the future tokens                               | masking upper triangular matrix, autoregressive property, unidirectional context           | You can't cheat by peeking at the next word.                           |
| **[[Decoder-Only]]**                    | Architecture dedicated to generation                           | no encoder-decoder cross-attention, stack of decoder blocks, input as prompt               | Pure generation machine: input -> process -> next token.               |
| **[[Last Token Embedding]]**            | Where the "meaning" of the sequence accumulates                | reading the vector of the last generated token, summarizing the prefix                     | In GPT, the last token holds the summary of everything before it.      |
| **[[Generation vs Understanding]]**     | Choosing the right tool                                        | GPT (Generation) vs BERT (Understanding/Classification), distinct use cases                | Use GPT to write; use BERT to read.                                    |
| **[[Evolution]]**                       | GPT-1 to GPT-4                                                 | zero-shot learning (GPT-2), few-shot (GPT-3), instruction tuning (InstructGPT), scaling    | From a decent sentence completer to a reasoning engine.                |
