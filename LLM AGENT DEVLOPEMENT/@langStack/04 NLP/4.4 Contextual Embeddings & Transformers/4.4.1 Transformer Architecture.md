| **Subtopic**                            | **Focus & Purpose**                                            | **Key Concepts / Details**                                                                 | **One-Line Recall**                                                    |
| --------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- |
| **[[4.4.1 Transformer Overview]]**      | The foundation of modern NLP (Attention is All You Need)       | encoder-decoder structure, parallelization, removing recurrence (RNNs), scalability        | The Transformer replaced loops with attention, enabling massive scale. |
| **[[Self-Attention]]**                  | Mechanism to relate words to each other in a sentence          | Query, Key, Value matrices, scaled dot-product attention, attention weights                | Every word looks at every other word to decide who is important.       |
| **[[Multi-Head Attention]]**            | Capture different types of relationships simultaneously        | multiple functional heads, concatenation, linear projections, learning diverse features    | One head focuses on syntax, another on semantics, another on position. |
| **[[Positional Encoding]]**             | Inject order information into the model                        | sine/cosine functions, learned embeddings, relative positioning, treating sequence as set  | Since Transformers see the whole sentence at once, they need a map of order.|
| **[[Feed-Forward Networks]]**           | Process information independently at each position             | pointwise MLP, ReLU/GELU activation, expanding and contracting dimensions                  | The brain power that processes the attention gathering.                |
| **[[Residuals & Norms]]**               | Facilitate deep network training                               | layer normalization, skip connections, preventing vanishing gradients, Add & Norm          | Shortcuts that let gradients flow through hundreds of layers.          |
