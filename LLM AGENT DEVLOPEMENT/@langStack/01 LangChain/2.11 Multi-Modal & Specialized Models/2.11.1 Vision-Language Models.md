
| **Subtopic** | **Focus & Purpose** | **Key Concepts / Details** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[2.11.1 Vision-Language Models]]** | Seeing AI | integrating models that can "see" and understand images | Giving eyes to the brain. |
| **[[GPT-4 Vision / Claude Vision]]** | The Models | using multi-modal LLMs that accept image inputs | The smartest eyes in the room. |
| **[[Image Input]]** | Mechanics | passing base64 strings or URLs to the model input | "Here is a picture of a cat, tell me what breed it is." |
| **[[Image Understanding]]** | Analysis | describing scenes, reading text in images, and identifying objects | "It looks like a red car parked on a steep hill." |
| **[[Visual Prompts]]** | Instruction | pointing at parts of an image (bounding boxes) in the prompt | "What is in the top-left corner?" |
| **[[Image Analysis Chains]]** | Workflow | combining vision models with logic chains to act on visual data | "If the screen shows an error, click the Reset button." |
| **[[Vision Integration]]** | Usage | standardizing image inputs across different providers | Plug-and-play sight. |
