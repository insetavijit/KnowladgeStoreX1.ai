
| **Subtopic** | **Focus & Purpose** | **Key Concepts / Details** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[2.11.6 Multi-Modal Embeddings]]** | Unification | mapping images and text to the same vector space | Comparing apples (images) to oranges (text). |
| **[[CLIP Embeddings]]** | The Model | OpenAI's model that learns visual concepts from natural language | The translator between pixels and words. |
| **[[Multi-Modal Vector Stores]]** | Storage | databases that can store and index vectors for both modalities | A bucket for *all* your data types. |
| **[[Cross-Modal Search]]** | Querying | using text to find images, or images to find text | "Find text documents that describe this photo." |
| **[[Image-Text Similarity]]** | Matching | calculating how closely a caption matches an image | "Is this a good caption?" |
| **[[Unified Representations]]** | Concept | treating all data as mathematical points in high-dimensional space | "Data is just numbers." |
