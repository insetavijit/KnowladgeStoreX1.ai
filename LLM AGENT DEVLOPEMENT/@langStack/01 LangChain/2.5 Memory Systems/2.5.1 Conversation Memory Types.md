
| **Subtopic** | **Focus & Purpose** | **Key Concepts / Details** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[2.5.1 Conversation Memory Types]]** | Strategies for retaining context | buffer, window, summary, token management | Different algorithms for deciding what to remember. |
| **[[ConversationBufferMemory]]** | Complete fidelity | storing raw text of all history, high token cost | Remembering everything exactly as it happened. |
| **[[ConversationBufferWindowMemory]]** | Recency focus | keeping only the last K interactions, forgetting old turns | Remembering only the recent conversation. |
| **[[ConversationSummaryMemory]]** | Semantic compression | using an LLM to summarize the conversation so far | Compressing history into a summary to save space. |
| **[[ConversationSummaryBufferMemory]]** | Hybrid approach | keeping recent raw text + old summary | Recent details are exact; old details are summarized. |
| **[[Selecting Memory Type]]** | Architecture decision | balancing cost, context window, and fidelity requirements | Choosing the right strategy for your specific use case. |
| **[[Memory Strategies]]** | Implementation patterns | stateless APIs vs stateful chains, managing session IDs | How to implement memory in a real application. |
