
| **Subtopic** | **Focus & Purpose** | **Key Concepts / Details** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[2.1.2 LLM Integration]]** | connecting application logic to language models | API keys, model selection, basic generation, error handling | The interface where code meets the AI model. |
| **[[LLM Wrappers]]** | Standard interface for text-in/text-out models | pure text completion models, `invocations`, legacy integration | Standardized wrapper for raw text completion models. |
| **[[ChatModels vs LLMs]]** | Distinct interfaces for chat vs completion | `SystemMessage`, `AIMessage`, `HumanMessage` vs raw strings | ChatModels use message lists; LLMs use raw text strings. |
| **[[Model Providers]]** | Integrating specific AI vendors | `ChatOpenAI`, `ChatAnthropic`, `Ollama`, `HuggingFace`, API config | Switching between different AI backends (OpenAI, Anthropic, Local). |
| **[[Model Parameters]]** | Controlling generation behavior | `temperature`, `top_p`, `max_tokens`, `stop_sequences` | Hyperparameters that control the creativity and length of outputs. |
| **[[Streaming]]** | Real-time response generation | `Stream` iterator, specific callbacks, token-by-token processing | Receiving generation output token-by-token for lower latency. |
| **[[Token Counting]]** | Application cost & context management | `tiktoken`, managing context windows, cost estimation | Tracking usage to manage costs and context window limits. |
