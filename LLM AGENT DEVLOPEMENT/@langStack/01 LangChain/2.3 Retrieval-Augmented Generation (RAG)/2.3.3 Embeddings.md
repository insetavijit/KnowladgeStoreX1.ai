
| **Subtopic** | **Focus & Purpose** | **Key Concepts / Details** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[2.3.3 Embeddings]]** | Converting text to numbers | vector representation, semantic search foundation | Turning words into lists of numbers so computers understand meaning. |
| **[[Embedding Models]]** | The engines of semantic search | models trained to map text to vector space (e.g., text-embedding-3) | The actual AI models that do the conversion. |
| **[[OpenAIEmbeddings]]** | The industry standard | standard API, high quality, pay-per-token | The default paid choice for high-quality embeddings. |
| **[[HuggingFaceEmbeddings]]** | Local and open-source | `sentence-transformers`, running locally, privacy-focused | Free, run-anywhere models from the open source community. |
| **[[Vector Representations]]** | Dense vector concepts | high-dimensional space, capturing semantic meaning | The mathematical concept of "meaning" as a point in space. |
| **[[Embedding Dimensions]]** | Vector size implications | 1536 (OpenAI) vs 768 (legacy), storage vs precision | How "wide" the vector is, affecting quality and cost. |
| **[[Cost-Performance Trade-offs]]** | Choosing the right model | MTEB leaderboard, latency vs accuracy vs price | Deciding if you need the smartest model or the fastest one. |
