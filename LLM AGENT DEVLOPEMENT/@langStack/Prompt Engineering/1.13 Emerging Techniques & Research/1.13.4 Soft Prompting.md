# 1.13.4 Soft Prompting

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.13.4.1 Discrete vs Continuous Prompts]]** | Definition | Text words vs Vector numbers | Hard prompts are English words; soft prompts are learned vector sequences. |
| **[[1.13.4.2 Learned Embeddings]]** | Training | Prepended tokens, virtual tokens | Finding a sequence of numbers that steers the model *better* than any English words could. |
| **[[1.13.4.3 Prompt Tuning (Lester et al.)]]** | Efficiency | Freezing weights, tuning input | Updating only the prompt vectors, which is 1000x cheaper than fine-tuning the model. |
| **[[1.13.4.4 P-Tuning]]** | Advanced | Trainable continuous prompts, LSTM usage | Using a small neural net to generate the virtual token embeddings. |
| **[[1.13.4.5 Parameter Efficiency]]** | PEFT | Low rank adaptation, resource constraints | Achieving fine-tuning results with a fraction of the parameters. |
| **[[1.13.4.6 Prefix Tuning]]** | Architecture | Appending vectors to every layer, not just input | A more powerful form of soft prompting that modifies activation at all depths. |
| **[[1.13.4.7 Transfer capability]]** | Portability | Moving soft prompts between tasks | Taking a "Politeness" soft prompt and applying it to a "Reasoning" task. |
| **[[1.13.4.8 Interpretability issues]]** | Black box | "What does this vector mean?", vector space | Human cannot read soft prompts; they are just arrays of floats. |
| **[[1.13.4.9 Applications]]** | Use cases | Domain adaptation, style transfer | rapidly adapting a frozen model to a new domain (e.g., medical). |
| **[[1.13.4.10 Future of Hard Prompts]]** | Speculation | "Will we still write English?", hybrid approach | Will engineers of the future write text or train vectors? |

This table represents a **complete guide to Soft Prompting**, passing numbers instead of words.
