# 1.13.5 Instruction Tuning

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.13.5.1 FLAN (Finetuned Language Net)]]** | History | Google's method, "More tasks = Better Generalization" | The insight that training a model on 1000 different tasks makes it good at tasks it has never seen. |
| **[[1.13.5.2 T0 and Multitask Training]]** | Scaling | BigScience, P3 dataset | Pushing the limits of how many different prompt variations we can train on. |
| **[[1.13.5.3 Dataset Diversity]]** | Data | "Not just Q&A, but creative writing too" | The performance of an instruction-tuned model is directly proportional to the variety of its training data. |
| **[[1.13.5.4 Zero-Shot Generalization]]** | Goal | Unseen tasks, novelty | The Holy Grail: A model that instantly understands a new game just by reading the rules. |
| **[[1.13.5.5 Natural Instructions]]** | Corpora | AI2 dataset, human-written instructions | Using real instructions written by humans rather than synthetic templates. |
| **[[1.13.5.6 Self-Instruct]]** | Method | Using AI to generate training data for AI | The bootstrapping method used to train Alpaca (GPT-3 generates the data, Llama learns from it). |
| **[[1.13.5.7 Alpaca / Vicuna]]** | Models | Stanford, UC Berkeley, open-source revolution | The proof that you don't need a massive cluster to train a chat model; you just need good data. |
| **[[1.13.5.8 Impact on Prompting]]** | Usage | "Why does it follow instructions?", alignment | Understanding that modern "prompting" only works because of this specific fine-tuning stage. |
| **[[1.13.5.9 Alignment]]** | Safety | Helpful, Honest, Harmless (HHH) | Instruction tuning is the first step in aligning the model with human intent. |
| **[[1.13.5.10 Catastrophic Forgetting]]** | Risk | "It forgot how to code", trade-offs | Sometimes making a model better at chatting makes it worse at math. |

This table represents a **complete guide to Instruction Tuning**, the reason Chatbots exist.
