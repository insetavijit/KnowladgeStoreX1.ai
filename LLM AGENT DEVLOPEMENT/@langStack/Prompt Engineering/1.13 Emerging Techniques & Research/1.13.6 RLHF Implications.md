# 1.13.6 RLHF Implications

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.13.6.1 Reward Modeling]]** | Training | Proxy model, "The Judge" | Training a separate AI to predict "Would a human like this answer?" |
| **[[1.13.6.2 Preference Data]]** | Input | A > B, pairwise ranking | Collecting millions of human choices to teach the model what "Good" looks like. |
| **[[1.13.6.3 PPO (Proximal Policy Optimization)]]** | Algo | Reinforcement learning, policy gradient | The mathematical algorithm used to update the model based on the reward score. |
| **[[1.13.6.4 Alignment Tax]]** | Cost | "Alignment tax", performance degradation | The phenomenon where enforcing safety often makes the model slightly dumber at creative tasks. |
| **[[1.13.6.5 Mode Collapse]]** | Risk | "Everything sounds the same", loss of variance | RLHF models tend to converge on a single "safe" style of speaking, killing creativity. |
| **[[1.13.6.6 Sycophancy]]** | Bias | "Yes sir, you are right", agreeing with errors | RLHF models learn that humans like to be agreed with, even when the human is wrong. |
| **[[1.13.6.7 Verbosity Bias]]** | Bias | "Longer is better", superficial quality | RLHF rewards length because humans mistakenly rate long answers as smarter. |
| **[[1.13.6.8 Training vs Prompting]]** | Limit | "You can't prompt away RLHF", deep grooves | If a behavior is baked in via RLHF (like refusing illegal acts), no amount of prompting will fix it. |
| **[[1.13.6.9 DPO (Direct Preference Optimization)]]** | New | Rafailov et al., simpler than PPO | A newer method that removes the need for a separate Reward Model (the future of alignment). |
| **[[1.13.6.10 KTO / IPO]]** | Variants | Kahneman-Tversky Optimization | Newer loss functions that attempt to align models using game theory. |

This table represents a **complete guide to RLHF**, the ghost in the machine.
