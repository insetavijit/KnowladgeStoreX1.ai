# 1.13.2 Automatic Prompt Engineering

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.13.2.1 APE (Zhou et al.)]]** | Concept | "Let the LLM write the prompt", Instruction generation | Asking LLM 1 to generate instruction candidates for LLM 2 to solve a task. |
| **[[1.13.2.2 Gradient-Based Optimization]]** | Math | Soft prompts, backpropagation | optimizing the prompt embeddings directly (like training weights) rather than editing text. |
| **[[1.13.2.3 Evolutionary Algorithms]]** | Selection | Mutation, crossover, survival of the fittest | Generating 50 prompt variants, keeping the top 5, and mutating them again. |
| **[[1.13.2.4 Prompt Breeding]]** | Iteration | "Mix Prompt A and Prompt B", genetic mixing | Combining the best traits of two high-performing prompts. |
| **[[1.13.2.5 Self-Refinement Loops]]** | Critique | "Critique this prompt and improve it" | Asking the model to act as a prompt engineer and fix its own instructions. |
| **[[1.13.2.6 Evaluation-Driven Search]]** | Metrics | Maximizing a score, hill climbing | The optimizer needs a clear metric (e.g., accuracy on a test set) to know if a change was good. |
| **[[1.13.2.7 DSPy Framework]]** | Tooling | Declarative prompting, compilable signatures | A Stanford framework that "compiles" intent into optimized prompts automatically. |
| **[[1.13.2.8 Prompt Mining]]** | Data | Extracting prompts from successful inputs | Looking at logs to see what users *actually* typed to get good results. |
| **[[1.13.2.9 Few-Shot Selection]]** | RAG | Finding the *best* examples to include | Automatically picking the 3 most relevant examples from a pool of 1000. |
| **[[1.13.2.10 Limitations of Auto-Prompting]]** | Reality | Local optima, unreadable prompts | Auto-generated prompts often look like gibberish to humans but work great for models. |

This table represents a **complete guide to Automatic Prompt Engineering**, machines teaching machines.
