# 1.13.8 Research Frontiers

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.13.8.1 System 2 Thinking]]** | Cognition | Slow thinking, Tree-of-Thought search | Teaching models to pause and simulate future states before committing to an answer. |
| **[[1.13.8.2 Neurosymbolic AI]]** | Hybrid | Neural nets + Symbolic logic, guaranteed correctness | combining the creativity of LLMs with the reliability of code/math solvers. |
| **[[1.13.8.3 Autonomous Agents]]** | Action | AutoGPT, BabyAGI, long horizons | Models that can pursue a goal for days, overcoming obstacles without human help. |
| **[[1.13.8.4 World Models]]** | Simulation | "Internal simulation", physics engine | Models that have an internal representation of how the physical world works. |
| **[[1.13.8.5 Interpretability]]** | Features | "Monosemanticity", Anthropic circuits | Understanding *which* neuron fires when you say "Eiffel Tower" to debias models. |
| **[[1.13.8.6 Continual Learning]]** | Memory | Avoiding catastrophic forgetting, live updates | Updating the model's knowledge daily without invalidating its previous skills. |
| **[[1.13.8.7 On-device LLMs]]** | Edge | 1B params, mobile execution, privacy | Running powerful models on your phone without sending data to the cloud. |
| **[[1.13.8.8 Energy Efficiency]]** | Green AI | Sparse models, MoE, 1-bit LLMs | Reducing the carbon footprint of training from "Small City" to "Small Car". |
| **[[1.13.8.9 Creativity & Novelty]]** | Definition | "True invention vs Remixing" | Research into whether models can actually invent new math or just remix old math. |
| **[[1.13.8.10 AGI Roadmaps]]** | Prediction | Scaling laws, "The Bitter Lesson" | The scientific debate on whether "More Compute" is all we need to reach AGI. |

This table represents a **complete guide to Research Frontiers**, the edge of the known universe.
