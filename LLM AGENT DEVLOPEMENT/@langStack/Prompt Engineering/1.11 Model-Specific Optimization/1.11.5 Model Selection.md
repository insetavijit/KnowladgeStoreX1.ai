# 1.11.5 Model Selection

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.11.5.1 Capability vs Cost]]** | Trade-off | "Smart, Fast, Cheap: Pick two" | The fundamental triangle of engineering decisions. |
| **[[1.11.5.2 Latency Requirements]]** | Speed | Real-time chat vs background batch | If the user is waiting, using a 70B model might be too slow regardless of quality. |
| **[[1.11.5.3 Privacy / Data Sovereignty]]** | Legal | GDPR, On-prem vs Cloud, Data retention | "Can I send this PII to OpenAI?" (Usually no, requiring local models). |
| **[[1.11.5.4 Context Window Needs]]** | Memory | 8k vs 128k vs 200k | If you need to summarize a book, GPT-3.5 (16k) physically cannot do it. |
| **[[1.11.5.5 Specialized Capabilities (Math/Code)]]** | Niche | CodeLlama, MathGPT, DeepSeek Coder | Some models are trained specifically for coding and beat general models 10x their size. |
| **[[1.11.5.6 Vendor Lock-in]]** | Risk | API compatibility, migration cost | "If OpenAI raises prices, can I switch to Anthropic easily?" |
| **[[1.11.5.7 Benchmarking for Task]]** | Testing | Custom evals, empirical data | Don't trust the leaderboard; run *your* prompts on the models to see which wins. |
| **[[1.11.5.8 Rate Limits]]** | Throughput | TPM (Tokens Per Minute), RPM | Can the provider actually handle your tragic load? |
| **[[1.11.5.9 Fine-tunability]]** | Customization | "Can I fine-tune this later?", adaptability | Some models (GPT-3.5, Llama) can be fine-tuned; others (GPT-4) cannot (easily). |
| **[[1.11.5.10 Future Proofing]]** | Strategy | Model obsolescence, stable APIs | "Will this model still exist in 6 months?" (API stability matter). |

This table represents a **complete guide to Model Selection**, the art of choosing the right tool.
