# 1.11.4 Open-Source Models

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.11.4.1 Llama 3 Prompt Format]]** | Syntax | `<|begin_of_text|>` tokens, specific templates | Open models are picky; if you miss the specific chat template string, they break. |
| **[[1.11.4.2 Mistral / Mixtral Nuances]]** | MoE | Mixture of Experts, prompt sensitivity | Understanding that Mixtral behaves differently than dense models. |
| **[[1.11.4.3 Chat Template Standards]]** | Tooling | `tokenizer.apply_chat_template()`, HuggingFace | Don't manually concat strings; use the tokenizer's standardized template function. |
| **[[1.11.4.4 Quantization Effects]]** | Performance | GGUF, GPTQ, precision loss | A 4-bit quantized model is dumber than the FP16 original; prompts might need to be simpler. |
| **[[1.11.4.5 Local Hosting Constraints]]** | Resources | VRAM limits, context length limits | You can't fit a 128k context into a consumer GPU; prompts must be tight. |
| **[[1.11.4.6 Fine-tuning Alignment]]** | Customization | LoRA, QLoRA, style transfer | Open models are meant to be fine-tuned; prompting is just the prototyping phase. |
| **[[1.11.4.7 Grammars / GBNF]]** | Control | Constrained decoding, forcing JSON | Using grammar files (GBNF) to force the local LLM to output *valid* syntax 100% of the time. |
| **[[1.11.4.8 Repetition Penalties]]** | Configuration | `repeat_penalty`, presence penalty | Local models get stuck in loops more often; tuning these params is essential. |
| **[[1.11.4.9 Stop Tokens]]** | Control | `EOS` token, custom stops | Manually defining where the model should shut up (e.g., "User:"). |
| **[[1.11.4.10 System Prompt Support]]** | Variability | "Some models ignore system prompts" | Not all open models are trained with a strong system prompt concept; testing is required. |

This table represents a **complete guide to Open-Source**, where you control the weights.
