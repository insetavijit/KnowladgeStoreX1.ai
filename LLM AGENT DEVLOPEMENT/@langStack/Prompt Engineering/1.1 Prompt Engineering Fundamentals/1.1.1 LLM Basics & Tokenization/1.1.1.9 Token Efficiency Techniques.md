# Token Efficiency Techniques

**Core Definition:** Token Efficiency is the art of maximizing model performance while minimizing token usage. Since tokens = cost and latency, reducing unnecessary tokens improves the speed and economic viability of LLM applications.

## Techniques for Optimization

### 1. Prompt Compression
Removing "fluff" words that don't add semantic value.
*   *Verbose:* "Hello, I would like to ask you if you could essentially summarize the following text for me so I can understand it better."
*   *Efficient:* "Summarize this text."

### 2. Reference by Index
Instead of repeating long names, assign them IDs in the context.
*   *Bad:* "Compare the iPhone 15 Pro Max to the Samsung Galaxy S24 Ultra..."
*   *Good:* "P1: iPhone 15 Pro Max. P2: Samsung S24 Ultra. Compare P1 and P2."

### 3. XML / Format Constraints
Using strict output schemas (like JSON) often forces the model to be more concise than free-form text.
*   *Prompt:* "Return only a JSON list."
*   *Result:* `["Topic A", "Topic B"]` (Very dense).

### 4. Few-Shot Optimization
Usually, we give 3-5 examples to teach a task.
*   **Technique:** Select the *shortest possible* examples that still demonstrate the pattern.
*   **Dynamic Selection:** Only insert examples that are semantically similar to the current user query (filtering from a database of 100 examples down to 3).

### 5. Content Filtering
Before sending a document to the LLM, use a cheaper script (or smaller LLM) to strip out navigation bars, HTML footers, and legal disclaimers.

## Trade-offs

| Efficiency Method | Risk |
| :--- | :--- |
| **Extreme Compression** | Loss of politeness/tone; model might become curt or miss nuance. |
| **removing Stop Words** | Grammar breaks; logic might degrade in complex reasoning tasks. |
| **Summarization** | "Lossy" compression; key details might be dropped from the summary. |

## Quick Summaries

**30-second version:**  
Token efficiency involves stripping unnecessary words, using concise formats (JSON), and dynamically selecting context. It reduces API bills and speeds up responses. However, over-optimizing can strip away the "logic traces" (Chain of Thought) that models need to reason effectively.

**One-line recall:**  
**More information per token = faster, cheaper, and often smarter systems (due to less noise).**

---

**Section:** **1.1.1.9 Token Efficiency Techniques**  
**Focus:** Prompt optimization  
**Last updated:** December 2025

---
