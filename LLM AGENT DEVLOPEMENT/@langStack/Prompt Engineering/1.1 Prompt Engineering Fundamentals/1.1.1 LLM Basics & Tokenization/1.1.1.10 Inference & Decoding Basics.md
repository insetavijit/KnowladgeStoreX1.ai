# Inference & Decoding Basics

**Core Definition:** Inference is the phase where the model generates text. **Decoding** is the specific algorithm chosen to select the "next token" from the probability list the model outputs. The choice of decoding strategy determines if the model is creative (random) or precise (deterministic).

## The Output is Probabilistic
The model doesn't output "The cat sat on the mat".
It outputs:
*   `mat`: 60%
*   `rug`: 25%
*   `couch`: 10%
*   `banana`: 0.001%

## Decoding Strategies

### 1. Greedy Decoding (Temperature = 0)
*   **Logic:** Always pick the #1 most likely token.
*   **Result:** Extremely repetitive, deterministic, and sometimes "boring". Good for math or code.

### 2. Sampling (Temperature > 0)
*   **Logic:** Pick the next token randomly, but weighted by probability.
*   **Temperature:** A knob that flattens or sharpens the probability curve.
    *   **Low Temp (0.2):** Makes the "winner" take all (Precision).
    *   **High Temp (0.8+):** Gives "underdogs" a chance (Creativity/Randomness).

### 3. Top-K Sampling
*   **Logic:** Only consider the top $K$ most likely words (e.g., Top 50). Discard the rest (to avoid choosing "banana" entirely). Then sample from those 50.

### 4. Top-P (Nucleus) Sampling
*   **Logic:** Pick the smallest set of top vocabulary words whose cumulative probability adds up to $P$ (e.g., 90%).
*   **Why:** It adapts dynamically. If the model is sure, the set is small (1 word). If unsure, the set is wide.

## Determinism
By default, LLMs are non-deterministic (because of hardware race conditions + sampling).
*   **Seed:** Some APIs allow you to set a `seed` parameter to force the same random choices every time (reproducibility).

## Quick Summaries

**30-second version:**  
The model outputs a list of likely words. **Decoding** decides which one to pick. **Greedy** picks the top one (robotic). **Sampling** rolls a dice. **Temperature** controls how "risky" the dice roll is. Use Low Temp for facts/code, High Temp for brainstorming/poetry.

**One-line recall:**  
**Temperature controls randomness; Top-P controls the pool of choices; Greedy is for exact answers.**

---

**Section:** **1.1.1.10 Inference & Decoding Basics**  
**Focus:** Generation behavior  
**Last updated:** December 2025

---
