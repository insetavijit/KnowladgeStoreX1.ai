# Embeddings (High-Level)

**Core Definition:** Embeddings are the way LLMs represent the *meaning* of text. They convert integer token IDs into massive lists of numbers (vectors) where similar concepts are located close to each other in mathematical space.

## The Concept of Vector Space

Imagine a 2D graph.
*   "Dog" might be at coordinate (2, 3).
*   "Cat" might be at (2, 4).
*   "Car" might be at (9, 9).

"Dog" is closer to "Cat" than "Car". LLMs use this same logic but in thousands of dimensions (e.g., 4096 dimensions).

## How It Works in an LLM

1.  **Input:** Token ID `[15496]` ("Hello").
2.  **Lookup:** The model has a giant "Embedding Matrix" (a lookup table). It finds row 15496.
3.  **Output:** That row contains a vector like `[0.12, -0.59, 0.99, ...]` (length 4096).

This vector captures the *semantic essence* of the word "Hello" in a way the neural network can perform math on.

## Semantic Algebra

Because meaning is now math, you can do "algebra" with concepts:
*   `King - Man + Woman ≈ Queen`
*   `Paris - France + Germany ≈ Berlin`

## Contextual Embeddings

In modern Transformers, embeddings are **contextual**.
*   **Static (Word2Vec):** The word "bank" has one vector, mixing "river bank" and "money bank".
*   **Contextual (LLM):** The Transformer adjusts the vector for "bank" based on the surrounding words ("river" or "money") as it passes through the layers. By the end of the model processing, the representation is purely specific to that instance.

## Why It Matters

1.  **RAG (Retrieval-Augmented Generation):** We use embeddings to search for documents. We turn a user query into a vector and find "nearest neighbor" documents in a database.
2.  **Semantic Search:** Keyword search fails on "automobile" vs "car". Embeddings understand they match.

## Quick Summaries

**30-second version:**  
Embeddings translate discrete token IDs into continuous numbered vectors. These vectors represent meaning. Similar concepts have similar vectors. This allows the model to understand nuances like synonyms and analogies mathematically.

**One-line recall:**  
**Embeddings turn words into coordinates in a meaning-space; closer coordinates = similar meanings.**

---

**Section:** **1.1.1.6 Embeddings (High-Level)**  
**Focus:** Semantic representation  
**Last updated:** December 2025

---
