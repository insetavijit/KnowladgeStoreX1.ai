# Practical Implications & Limitations

**Core Definition:** Understanding the underlying mechanics of tokenization, transformers, and training reveals specific **failure modes** and architectural limitations that prompt engineers must work around. These are not "bugs" but fundamental properties of the technology.

## 1. Math and Counting (Token Blindness)
*   **Issue:** Models see token IDs, not digits.
*   **Result:** They are bad at character-level tasks (wordle, crosswords) and large number arithmetic (multiplying 5-digit numbers).
*   **Fix:** Use Python (Code Interpreter) for math; don't ask the LLM to calculate in its head.

## 2. Hallucination (Probabilistic Nature)
*   **Issue:** The model is trained to *complete patterns*, not to verify facts. If the most probable continuation is a convincing lie, checks are needed.
*   **Result:** Convincingly citing non-existent court cases or papers.
*   **Fix:** Grounding (RAG) and Citation prompting ("Answer only using the provided text").

## 3. Context Forgetting (Attention Span)
*   **Issue:** "Lost in the Middle" phenomenon.
*   **Result:** Instructions buried in paragraph 40 of a 50-paragraph document might be ignored.
*   **Fix:** Put critical instructions at the **End** (Recency bias) or **Beginning** (Primacy bias) of the prompt.

## 4. Sensitivity (Brittleness)
*   **Issue:** Changing a single word can shift token boundaries and completely change the semantic embedding output.
*   **Result:** A prompt works 95% of the time, then fails on a synonym.
*   **Fix:** Evaluation suites (Test prompts on 50 examples, not just 1).

## 5. Cutoff Dates (Static Weights)
*   **Issue:** Training ends on a specific date. The model weights are static.
*   **Result:** It doesn't know about yesterday's news.
*   **Fix:** RAG (Search tools) to inject fresh context.

## Quick Summaries

**30-second version:**  
LLMs are statistical engines, not logic engines. They struggle with precise math (due to tokenization), facts (due to probabilistic generation), and massive context (due to attention limits). Good engineering involves offloading weak areas to external tools (calculators, databases) and structuring prompts to play to the model's strengths.

**One-line recall:**  
**Don't ask an LLM to do a Calculator's job (math) or a Database's job (fact storage); use it as a Reasoning Engine.**

---

**Section:** **1.1.1.11 Practical Implications & Limitations**  
**Focus:** System-level impact  
**Last updated:** December 2025

---
