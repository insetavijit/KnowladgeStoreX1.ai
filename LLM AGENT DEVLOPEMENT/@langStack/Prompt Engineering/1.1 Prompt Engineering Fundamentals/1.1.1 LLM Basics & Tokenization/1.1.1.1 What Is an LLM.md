# What Is an LLM?

**Core definition:** A **Large Language Model (LLM)** is a machine learning model trained on vast amounts of text data to **predict the next token** in a sequence, enabling it to generate coherent, context-aware language.

**Key idea:**

- **Input:** A sequence of tokens (text broken into units)
    
- **Output:** A probability distribution over possible next tokens
    
- **Generation:** Repeated next-token prediction produces fluent text
    

## Why It Works

LLMs do not store facts or rules explicitly. Instead, they learn **statistical patterns** in language—grammar, semantics, reasoning traces—by optimizing for next-token prediction across massive datasets.

This simple objective scales into powerful behavior when combined with:

- Large model size (parameters)
    
- High-quality, diverse training data
    
- Deep neural architectures (Transformers)
    

## Essential Characteristics

**1. Probabilistic generation:**  
Outputs are not deterministic by default; the model samples from likely next tokens based on learned probabilities.

**2. Generative modeling:**  
LLMs generate new text rather than retrieving fixed answers, enabling summarization, explanation, reasoning, and creativity.

**3. Context sensitivity:**  
Predictions depend on the entire provided context window, allowing continuity, reference resolution, and structured responses.

**4. Token-based reasoning:**  
All understanding and output—words, numbers, code—are processed as tokens, not symbolic concepts.

**5. No inherent goals or memory:**  
An LLM alone is **stateless** and reactive; it responds only to the current input.

## How Next-Token Prediction Looks

```
Input tokens → Model → P(next_token | context)
                    ↓
               sample / select
                    ↓
               append token → repeat
```

Through repetition, this process yields paragraphs, code, dialogue, or structured data.

## What an LLM Is vs Is Not

**LLM is:**

- A language probability engine
    
- A general-purpose text generator
    
- A pattern learner from data
    

**LLM is not:**

- A database of facts
    
- A reasoning engine by default
    
- An autonomous system
    

(Autonomy emerges only when wrapped in control logic—i.e., agents.)

## Common Misconceptions

|Misconception|Reality|
|---|---|
|“LLMs understand language like humans”|They model statistical patterns, not meaning|
|“LLMs always give the correct answer”|Outputs reflect likelihood, not truth|
|“LLMs reason symbolically”|Reasoning is implicit, token-based, and approximate|
|“LLMs remember past chats”|Memory must be provided externally|

## Relationship to Agents

- **LLM alone:** One input → one output
    
- **LLM in a system:** Can be combined with memory, tools, and control loops
    
- **LLM agent:** LLM + orchestration = goal-driven behavior
    

Understanding LLMs is foundational before studying agents, workflows, or multi-agent systems.

## Quick Summaries

**30-second version:**  
An LLM is a generative model trained to predict the next token in text. By repeating this prediction across context, it produces fluent language, code, and structured outputs—but it has no goals, memory, or agency on its own.

**One-line recall:**  
**LLM = large neural network trained for next-token prediction → coherent language generation**

---

**Section:** **1.1.1.1 What Is an LLM**  
**Focus:** Core concept  
**Last updated:** December 2025

---
