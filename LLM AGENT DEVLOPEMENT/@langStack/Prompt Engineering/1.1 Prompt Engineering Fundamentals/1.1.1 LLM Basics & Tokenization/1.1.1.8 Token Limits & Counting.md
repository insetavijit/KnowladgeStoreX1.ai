# Token Limits & Counting

**Core Definition:** Token limits are the hard constraints imposed by the model provider or hardware. They govern **Input limits** (how much you can read) and **Output limits** (how much you can write). Accurately counting tokens is essential for preventing API errors and managing costs.

## Types of Limits

1.  **Context Window Limit (Total Limit):**
    *   $Limit = InputTokens + OutputTokens$
    *   If you fill the context with 8000 tokens of input, and the limit is 8192, the model can only generate 192 tokens of output before cutting off.

2.  **Max Output Limit (Max Completion Tokens):**
    *   Often separate from the context limit.
    *   *Example:* GPT-4-Vision might have 128k context but cap output at 4096 tokens. You cannot generate a whole book in one go, even if the context window allows it.

## Counting Tokens

Since you are billed by the token and limits trigger errors, you must count them *before* sending a request.

*   **Python Library (`tiktoken` for OpenAI):**
    ```python
    import tiktoken
    enc = tiktoken.encoding_for_model("gpt-4")
    count = len(enc.encode("Hello world"))
    ```

*   **Heuristics (Rough Estimation):**
    *   Char count / 4 ≈ Token Count
    *   Word count * 1.3 ≈ Token Count

## Handling Overflow

When `Input + ExpectedOutput > Limit`, you must:
1.  **Drop History:** Remove oldest chat messages.
2.  **Summarize:** Replace old history with a summary.
3.  **RAG:** Retrieve only relevant snippets instead of stuffing the whole document in.

## Cost Implications

Pricing is usually:
*   **Input Tokens:** Cheaper (e.g., $5 / 1M).
*   **Output Tokens:** More expensive (e.g., $15 / 1M) because generation is computationally heavier (sequential).

## Quick Summaries

**30-second version:**  
Limits apply to the sum of Input + Output. Output is often capped separately (and lower) than input. You must count tokens using model-specific tokenizers (not `split(' ')`) to avoid `context_length_exceeded` errors.

**One-line recall:**  
**Total Limit = Prompt + Completion; Generation stops abruptly when the Output limit or Total limit is hit.**

---

**Section:** **1.1.1.8 Token Limits & Counting**  
**Focus:** Operational constraints  
**Last updated:** December 2025

---
