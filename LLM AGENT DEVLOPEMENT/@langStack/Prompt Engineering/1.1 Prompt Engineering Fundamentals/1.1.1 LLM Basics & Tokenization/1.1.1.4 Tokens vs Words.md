# Tokens vs Words

**Core Definition:** LLMs do not read text as words or characters. they read **Tokens**. A token is the fundamental unit of information for an LLM/ It can be a whole word, part of a word, or even a space/symbol. This distinction is vital because it explains why LLMs struggle with simple tasks like counting letters or rhyming.

## What is a Token?

A token is a chunk of text mapped to a unique number (Integer ID).

*   **Common words:** Often 1 token (e.g., "apple", " the").
*   **Complex words:** Split into multiple tokens (subwords).
*   **Whitespace:** Usually part of the token (e.g., " hello" is different from "hello").

## Typical Tokenizer Behavior

Modern tokenizers (like Byte-Pair Encoding or BPE) aim for efficiency. They merge frequent character pairs into single tokens.

| Text Input | Tokenization (Approximate) | Token Count |
| :--- | :--- | :--- |
| "apple" | `[apple]` | 1 |
| "Apple" | `[Apple]` | 1 (Case sensitive) |
| "friendship" | `[friendship]` | 1 |
| "unfriendly" | `[un]`, `[friendly]` | 2 |
| "12345" | `[12]`, `[345]` (varies) | 2 or 3 |

**Rule of Thumb:**  
**1000 tokens â‰ˆ 750 words** (in English).

## The "Strawberry" Problem

Why can't GPT-4 count how many 'r's are in "Strawberry"?
*   **Human view:** S-t-r-a-w-b-e-r-r-y (Letters)
*   **LLM view:** `[Straw]` `[berry]` (Tokens)

The model literally does not "see" the letters inside the token ID. It only knows the statistical properties of the token `[berry]`. It has to "hallucinate" the spelling based on training data associations unless you force it to break the word down (e.g., "spell it out").

## Why Not Just Use Words?
1.  **Vocabulary Size:** English has hundreds of thousands of words. Including forms like "run", "running", "ran", "runs" wastes space. Subwords (`run`, `ning`, `s`) are more efficient.
2.  **Unknown Words:** If a model only knew whole words, it would fail on a new word like "iPhone16". With tokens, it can break it into `[iPhone]`, `[16]`.

## Quick Summaries

**30-second version:**  
LLMs process **Tokens**, not words. A token is a cluster of characters. Common words are single tokens; rare words are split into sub-tokens. This abstraction is efficient but causes "blind spots" where the model cannot "see" individual characters inside a token (like counting letters).

**One-line recall:**  
**LLMs see the world in chunks (tokens), not characters; roughly 0.75 words per token.**

---

**Section:** **1.1.1.4 Tokens vs Words**  
**Focus:** Representation units  
**Last updated:** December 2025

---
