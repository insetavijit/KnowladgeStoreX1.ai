# 1.1.8 Common Pitfalls

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.1.8.1 Hallucinations & Fabrication]]** | Factuality failure | Grounding, citation requirements, retrieval reliance, confidence scoring | Models will confidently invent plausible-sounding falsehoods if not strictly grounded or constrained. |
| **[[1.1.8.2 Ambiguity & Vague Intent]]** | Instruction failure | Specificity gap, multiple valid interpretations, under-specification | Vague instructions force the model to guess your intent, leading to inconsistent or undesirable outputs. |
| **[[1.1.8.3 Information Overload]]** | Context failure | Lost in the middle, attention dilution, irrelevant noise | Flooding the context with too much data causes the model to miss critical instructions or mix up facts. |
| **[[1.1.8.4 Conflicting Instructions]]** | Logic failure | Paradoxes, negative constraints vs positive examples, overriding rules | Giving the model logically impossible or contradictory rules leads to unpredictable behavior. |
| **[[1.1.8.5 Model Bias & Stereotyping]]** | Safety failure | Representation bias, uncensored outputs, cultural assumptions | Unchecked models can reproduce harmful stereotypes or biases present in their training data. |
| **[[1.1.8.6 Prompt Injection & Jailbreaking]]** | Security failure | Ignore previous instructions, DAN mode, delimit injection | Neglecting security allows malicious users to hijack the model's behavior or extract system prompts. |
| **[[1.1.8.7 Over-Constraining]]** | Performance failure | Brittleness, "do too many things", cognitive load | Adding too many rigid constraints can degrade the model's reasoning ability, causing it to fail everything. |
| **[[1.1.8.8 Negation Handling ("Don't")]]** | Instruction adherence | Positive reframing, focus on "do" vs "don't" | LLMs often struggle with "don't" instructions; telling them what *to* do is more reliable than what *not* to do. |
| **[[1.1.8.9 Assumed Knowledge]]** | Context gap | Implicit context, undefined acronyms, relying on training cutoff | Assuming the model knows specific internal acronyms or recent events without context leads to errors. |
| **[[1.1.8.10 Inconsistent Formatting]]** | Parsing failure | One-off errors, trailing commas, markdown leakage | Failing to enforce a strict output schema results in broken pipelines when downstream code fails to parse the response. |

This table represents a **complete guide to Common Pitfalls**, helping developers preemptively solve the most frequent causes of failure.
