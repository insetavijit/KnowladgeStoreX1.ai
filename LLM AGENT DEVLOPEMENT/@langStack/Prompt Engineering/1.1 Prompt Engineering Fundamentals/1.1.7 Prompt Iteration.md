# 1.1.7 Prompt Iteration

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.1.7.1 Prompt Version Control]]** | Change management | Git for prompts, changelogs, version tagging, rollbacks | Treating prompts as code (with versions and commit history) prevents regression and confusion. |
| **[[1.1.7.2 A/B Testing Prompts]]** | Empirical comparison | Split testing, conversion rate, side-by-side evaluation, ELO rating | Running multiple prompt versions in parallel against live traffic proves which one performs better. |
| **[[1.1.7.3 The Iterative Refinement Loop]]** | Workflow cycle | Draft → Test → Analyze → Refine → Repeat | Prompt engineering is not a one-time write; it is a continuous cycle of observation and improvement. |
| **[[1.1.7.4 Evaluation Metrics (Rubrics)]]** | Quality measurement | Cosine similarity, BLEU/ROUGE (less useful), LLM grading, exact match | Defining clear success metrics is the only way to objectively measure prompt improvement. |
| **[[1.1.7.5 LLM-as-a-Judge]]** | Automated scoring | GPT-4 evaluating GPT-3.5, scoring rubrics, critique prompts | Using a stronger model to grade the outputs of a faster model scales evaluation without human bottleneck. |
| **[[1.1.7.6 Few-Shot Selection Strategy]]** | Example curation | Dynamic selection, diversity sampling, negative examples | Iteratively refining which examples are included in the prompt often yields higher gains than changing instructions. |
| **[[1.1.7.7 Regression Testing]]** | Stability assurance | Golden datasets, unit tests for prompts, CI/CD pipelines | Ensuring new prompt improvements don't break previously working behaviors ("whack-a-mole"). |
| **[[1.1.7.8 Analyzing Failure Modes]]** | Debugging | Edge case analysis, error categorization, token inspection | Systematically categorizing where the prompt fails (e.g., "ignored constraint" vs "hallucinated") guides the next fix. |
| **[[1.1.7.9 User Feedback Integration]]** | Ground truth | Thumbs up/down, corrections, human-in-the-loop tuning | Real user feedback is the ultimate signal for prompt performance and should feed back into the iteration loop. |
| **[[1.1.7.10 Automated Prompt Engineering (APE)]]** | AI-driven optimization | DSPy, auto-optimization frameworks, gradient descent for prompts | Letting an LLM rewrite its own prompts based on performance data can outperform human intuition. |

This table represents a **complete guide to Prompt Iteration**, shifting the practice from "guessing" to reliable "engineering."
