| **Section**                                           | **Focus**               | **Key Utilities / Concepts**                                                             | **One-Line Recall**                                                                          |
| ----------------------------------------------------- | ----------------------- | ---------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- |
| **[[1.1.1.1 What Is an LLM?]]**                       | Core concept            | large language models, next-token prediction, generative modeling, probabilistic outputs | An LLM predicts the next token to generate coherent language from data.                      |
| **[[1.1.1.2 How LLMs Are Trained]]**                  | Training lifecycle      | pretraining, self-supervised learning, fine-tuning, SFT, RLHF, base vs chat models       | LLMs are pretrained on vast data and fine-tuned to follow instructions and align with users. |
| **[[1.1.1.3 Transformer Architecture Overview]]**     | Model internals         | self-attention, layers, embeddings, positional encoding, parallelism                     | Transformers use self-attention to model relationships between tokens efficiently.           |
| **[[1.1.1.4 Tokens vs Words]]**                       | Representation units    | subwords, BPE, WordPiece, SentencePiece, byte-level tokens                               | LLMs operate on tokens rather than words to flexibly encode any text.                        |
| **[[1.1.1.5 Tokenization Process]]**                  | Text preprocessing      | normalization, vocabulary, encode/decode, token IDs                                      | Tokenization converts raw text into token IDs the model can process.                         |
| **[[1.1.1.6 Embeddings (High-Level)]]**               | Semantic representation | vector embeddings, contextual meaning, similarity space                                  | Tokens are mapped to vectors that capture semantic meaning in context.                       |
| **[[1.1.1.7 Context Window & Memory]]**               | Working memory          | max context length, truncation, recency bias, sliding window                             | The context window limits how much text an LLM can attend to at once.                        |
| **[[1.1.1.8 Token Limits & Counting]]**               | Operational constraints | input/output budgets, tokenizers, estimation tools, overflow handling                    | Token limits cap prompt size, and counting tokens prevents errors and cost surprises.        |
| **[[1.1.1.9 Token Efficiency Techniques]]**           | Prompt optimization     | concise prompts, compression, summarization, chunking, few-shot trade-offs               | Efficient prompts maximize information while minimizing token usage.                         |
| **[[1.1.1.10 Inference & Decoding Basics]]**          | Generation behavior     | greedy vs sampling, temperature, top-k/top-p, determinism vs creativity                  | Decoding strategies control how deterministic or creative model outputs are.                 |
| **[[1.1.1.11 Practical Implications & Limitations]]** | System-level impact     | cost, latency, truncation risk, multilingual quirks, token brittleness                   | Tokenization and limits directly shape reliability, cost, and behavior of LLM systems.       |

This table now represents a **complete, interview-ready, and system-aware topic breakdown** for _LLM Basics & Tokenization_, suitable as the foundation for all subsequent LLM and agent development sections.