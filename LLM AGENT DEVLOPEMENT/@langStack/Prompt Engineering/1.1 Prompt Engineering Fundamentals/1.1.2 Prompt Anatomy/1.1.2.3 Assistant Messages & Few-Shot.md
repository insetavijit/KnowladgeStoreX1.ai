# Assistant Messages & Few-Shot

**Core definition:** The **Assistant Message** is a prompt component that simulates a previous response from the model. It is primarily used to provide examples (**few-shot learning**) or to "pre-fill" the beginning of a response to guide the output.

**Key idea:**

-   **Mocking History:** You can insert "fake" assistant messages into the chat history to trick the model into thinking it has already answered in a certain way.
-   **Style Transfer:** By showing how the "Assistant" responded previously, you define the tone and format implicitly.
-   **Output Steering:** Starting an assistant message with a specific prefix forces the model to continue from that point.

## Why It Matters

Instructions alone (zero-shot) are often insufficient for complex tasks. LLMs are pattern matchers; seeing an example (Assistant Message) of the *desired* output is often more powerful than reading a description of it.

## Essential Functions

**1. Few-Shot Learning (Examples):**
Providing input-output pairs to teach the task.
*   User: "Convert to JSON: Name=John, Age=30"
*   Assistant: `{"name": "John", "age": 30}`
*   User: "Convert to JSON: Name=Mary..." (Model now follows the pattern)

**2. Pre-filling / Priming:**
Forcing the start of the response.
*   User: "Write a Python script to scrape a website."
*   Assistant: "Here is a Python script using BeautifulSoup to scrape a website: ```python"
*   *Result:* The model immediately writes code, skipping the "Sure! I can help with that" chatter.

**3. Format Anchoring:**
Demonstrating complex schemas that are hard to describe.

## Anatomy of Few-Shot Prompting

A standard few-shot prompt structure using message roles:

```json
[
  {"role": "system", "content": "You are a sentiment analyzer. Answer with positive/negative only."},
  {"role": "user", "content": "I loved the movie!"},
  {"role": "assistant", "content": "Positive"},
  {"role": "user", "content": "The food was terrible."},
  {"role": "assistant", "content": "Negative"},
  {"role": "user", "content": "It was okay, I guess."}
]
```
*The model is now highly likely to output just "Neutral" or a single word label.*

## System vs Assistant for Examples

| Strategy | Pros | Cons |
| :--- | :--- | :--- |
| **Examples in System Prompt** | Keeps context clean; global visibility. | Can be "diluted" by long conversations. |
| **Fake History (User/Assistant Pairs)** | Strongest adherence; leverages conversation priors. | Consumes more tokens; potential for confusion if not clear. |

## Common Patterns

**1. The "Sure, here is..." Pre-fill**
To avoid refusals or filler text, inject an Assistant message that has already "agreed" to the task.

**2. The 1-Shot Anchor**
Even a single example (1-shot) significantly improves adherence to JSON schemas or specific styles compared to 0-shot.

**3. Negative Examples**
Showing an Assistant message behaving *incorrectly* (and labeled as such, though tricky) or showing a correction sequence. (Usually positive examples are safer).

## Quick Summaries

**30-second version:**
Assistant Messages usually represent the model's output. However, in prompt engineering, we "stub" or "mock" these messages to provide examples (few-shot prompting) or to steer the generation. By manually inserting an Assistant message, we show the model exactly *how* it should have responded, creating a pattern for it to follow for the actual query.

**One-line recall:**
**Assistant Message = Examples (Few-Shot) + Response Priming.**

---

**Section:** **1.1.2.3 Assistant Messages & Few-Shot**
**Focus:** Example-based learning
**Last updated:** December 2025

---
