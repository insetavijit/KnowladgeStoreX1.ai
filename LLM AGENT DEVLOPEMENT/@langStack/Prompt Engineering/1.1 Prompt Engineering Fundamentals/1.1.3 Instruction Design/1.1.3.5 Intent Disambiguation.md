# Intent Disambiguation

**Core definition:** **Intent Disambiguation** is the process of clarifying the user's underlying goal *before* or *during* the generation process to prevent the model from making incorrect assumptions.

**Key idea:**
-   **Assumption Avoidance:** LLMs are trained to be helpful, so they will often guess your intent rather than ask for clarification.
-   **Proactive Clarification:** Good prompts explicitly resolve potential conflicts (e.g., "If X is unclear, do Y").
-   **Interactive Disambiguation:** Asking the model to ask *you* questions.

## Why It Matters

If you ask "How do I beat the level?", the intent depends entirely on the game. The model might assume you mean *Super Mario* because it's popular. Disambiguation ensures the model knows you are playing *Dark Souls*. It is the safety net for Ambiguity (1.1.3.1).

## Essential Utilities

**1. The "Clarification" Instruction:**
"If the request is unclear, ask clarifying questions before answering."

**2. The Conditional Branching:**
"If the text is legal, summarize it. If it is medical, translate it."

**3. The "Assumption Check":**
"State your assumptions before solving the problem."

## Anatomy of Disambiguation

**Scenario: "Draft a contract."**

**Ambiguous Path:**
Model writes a generic Service Agreement (assuming business context).

**Disambiguated Prompt:**
> "Draft a contract.
> Note: This is for a specific **Residential Lease** [Context].
> If any terms regarding 'pets' or 'subletting' are missing, stop and ask me for details [Stop Condition].
> Do not assume standard clauses apply."

## Common Patterns

**1. The "Refusal to Guess"**
"Do not make up facts. If the answer is not in the context, say 'I don't know'."

**2. The "Multiple Choice" Strategy**
"Did you mean A, B, or C? Please select one." (Useful for chatbots).

**3. The "Step-Back" Prompt**
"Restate my request in your own words to confirm you understand my intent." (Verification step).

## Quick Summaries

**30-second version:**
Intent Disambiguation is about synchronizing the user's mental model with the AI's execution plan. Because LLMs are "people pleasers," they will hallucinate an intent if one isn't provided. Disambiguation techniques force the model to pause, check its assumptions, or ask for help, rather than charging ahead with a wrong guess.

**One-line recall:**
**Disambiguation = clarifying assumptions BEFORE generation.**

---

**Section:** **1.1.3.5 Intent Disambiguation**
**Focus:** Error Prevention
**Last updated:** December 2025

---
