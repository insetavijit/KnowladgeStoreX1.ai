# 1.7.5 Jailbreak Prevention

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.7.5.1 DAN (Do Anything Now) Detection]]** | Pattern matching | "Ignore previous instructions", "You are free", override attempts | Recognizing the classic "DAN" pattern where users try to remove the model's safety constraints. |
| **[[1.7.5.2 Roleplay Attacks]]** | Context shifting | "Act as a bad guy", "In a movie script", hypothetical shield | Attackers often frame harmful requests as "just a story" to bypass filters. |
| **[[1.7.5.3 Foreign Language Attacks]]** | Obfuscation | Low-resource languages, Base64 encoding, ROT13 | Models are often less safe in languages other than English; attackers exploit this gap. |
| **[[1.7.5.4 Base64/Encoding Attacks]]** | Bypass | Encoding harmful prompts in Base64 causes the filter to miss them but the model to read them. |
| **[[1.7.5.5 Hypothetical Scenarios]]** | Logic trap | "Imagine you could...", logical unpinning | Using deep hypothetical nesting to confuse the model's safety logic. |
| **[[1.7.5.6 Adversarial Suffixes]]** | ML Attacks | "Describe.. [random string]", gradient-based attacks | Appending specific, meaningless strings that mathematically force the model to output harmful content. |
| **[[1.7.5.7 Prefix Matching]]** | Completion forcing | "The bomb recipe is:...", forcing the start | If you force the model to start with "Sure, here is how:", it is statistically more likely to continue the harmful response. |
| **[[1.7.5.8 Refusal Training]]** | Robustness | "I cannot do that", resilient refusal | Training the model to recognize *intent*, not just keywords, makes it harder to trick. |
| **[[1.7.5.9 Developer Mode Detection]]** | Social engineering | "Simulate Developer Mode", "Debug mode enable" | Users pretending to be OpenAI developers to override safety settings. |
| **[[1.7.5.10 System Integrity Checks]]** | Monitoring | "Did the system prompt change?", real-time alerts | Detecting if the core instructions have been successfully overwritten by a user. |

This table represents a **complete guide to Jailbreak Prevention**, the cat-and-mouse game of AI security.
