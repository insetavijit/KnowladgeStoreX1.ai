# 1.7.4 Prompt Injection Defense

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.7.4.1 Delimiters]]** | Segmentation | `"""`, `---`, `<user_input>`, clear boundaries | Using clear symbols to separate "System Instructions", "Context", and "User Input" prevents bleeding. |
| **[[1.7.4.2 Instruction Hierarchy]]** | Priority | "System overrides User", "Ignore conflicting instructions" | Explicitly stating that system instructions are immutable and take precedence over user text. |
| **[[1.7.4.3 Sandwich Defense]]** | Structure | Human input between two sets of system instructions | Placing instructions *after* the user input reinforces the rules just before generation. |
| **[[1.7.4.4 Input Sanitization]]** | Cleaning | Stripping control characters, limiting length, quote escaping | Neutralizing potential code or script vectors before they reach the model. |
| **[[1.7.4.5 Output Validation]]** | Check | "Does output contain secret?", canary check | Checking the output to ensure it hasn't leaked the system prompt or sensitive data. |
| **[[1.7.4.6 XML Tagging]]** | Encapsulation | `<command>User Input</command>`, structured prompts | Wrapping user input in descriptive XML tags helps the model distinguish data from code. |
| **[[1.7.4.7 Canaries]]** | Detection | Leaking secret strings, honeypots | Placing a random string in the prompt and checking if it appears in the output signals a leak. |
| **[[1.7.4.8 Random Strings]]** | Uniqueness | Dynamic delimiters (`###_RANDOM_###`) | Using randomly generated delimiters for each request prevents users from guessing the separator. |
| **[[1.7.4.9 Separate LLM Review]]** | Architecture | "Did the user try to hack?", adversarial detector | Using a separate, cheaper model to just check "Is this prompt an attack?" before processing. |
| **[[1.7.4.10 Parameter Isolation]]** | Tooling | `system`, `user`, `assistant` roles (API level) | Using the API's role parameters is safer than concatenating everything into one big string. |

This table represents a **complete guide to Prompt Injection Defense**, securing the LLM against hackers.
