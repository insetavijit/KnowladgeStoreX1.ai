# 1.5.7 Chunking Strategies

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.5.7.1 Character/Token Split]]** | Naive method | `TextSplitter`, fixed length, truncation | Simply cutting text every 500 characters is fast but often breaks sentences and meaning. |
| **[[1.5.7.2 Sentence Splitting]]** | Linguistic | NLP tokenization, "split at .", NLTK/Spacy | Splaitting by sentence ensures grammatical correctness but leads to variable chunk sizes. |
| **[[1.5.7.3 Paragraph/Structural Split]]** | Document structure | Double newline `\n\n`, markdown headers, section awareness | Respecting the author's structure (paragraphs) usually yields the most semantically coherent chunks. |
| **[[1.5.7.4 Semantic Chunking]]** | Embedding-based | Cosine similarity change, topic boundaries, dynamic split | splitting text only when the topic changes (detected by embedding shifts) creates highly coherent chunks. |
| **[[1.5.7.5 Recursive Character Text Splitter]]** | Best practice | "Try paragraphs, then sentences, then chars", LangChain default | An algorithm that tries to split by largest separators first (`\n\n`), falling back to smaller ones (`.`) only if needed. |
| **[[1.5.7.6 Markdown/Code Aware Chunking]]** | Syntax specific | Splitting by class/function, header hierarchy | Chunking code by functions (not lines) prevents breaking the syntax; Markdown splitting preserves header context. |
| **[[1.5.7.7 Overlap Management]]** | Context bridging | "50 token overlap", rolling window, continuity | Including the last 50 tokens of chunk A at the start of chunk B prevents losing meaning at the boundary. |
| **[[1.5.7.8 Parent-Child Chunking]]** | Retrieval optimization | Indexing small chunks, returning big chunks | Searching on small, specific sentences but returning the whole surrounding paragraph (Parent) for context. |
| **[[1.5.7.9 Topic-Based Segmentation]]** | Clustering | LDA, keyword grouping, semantic clustering | Grouping non-contiguous sentences that discuss the same topic into a single synthetic chunk. |
| **[[1.5.7.10 Optimal Chunk Size]]** | Tuning | Granularity trade-off, 256 vs 512 vs 1024 tokens | Small chunks are precise but lack context; large chunks have context but noise. 512 is a common sweet spot. |

This table represents a **complete guide to Chunking Strategies**, the invisible art that makes or breaks RAG.
