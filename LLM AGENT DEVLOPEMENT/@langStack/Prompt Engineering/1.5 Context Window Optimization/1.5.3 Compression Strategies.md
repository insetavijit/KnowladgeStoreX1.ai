# 1.5.3 Compression Strategies

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.5.3.1 Abstractive Summarization]]** | Rewriting | Generating new concise text, "TL;DR", synthesis | Asking an LLM to "rewrite this shorter" creates frequent-dense summaries. |
| **[[1.5.3.2 Extractive Summarization]]** | Selecting | Highlighting, picking key sentences, bullet points | Selecting the most important existing sentences is faster and less prone to hallucination than rewriting. |
| **[[1.5.3.3 Key-Value Extraction]]** | Structured compression | JSON conversion, tabular data | Converting a verbose paragraph into a clean `{ "key": "value" }` pair drastically reduces token count. |
| **[[1.5.3.4 Soft Prompts (Vectors)]]** | Embeddings | Learned vectors, prefix tuning, hyper-compression | (Advanced) Using learned vector representations instead of raw text triggers model states efficiently. |
| **[[1.5.3.5 Removing Stop Words]]** | Syntactic compression | "The", "and", "is", NLP preprocessing | Removing grammatical glue words often leaves the semantic meaning intact while saving 20-30% tokens. |
| **[[1.5.3.6 Syntax Compression]]** | Formatting | Minifying JSON, removing whitespace, short delimiters | Removing unnecessary whitespace and formatting chars from data structures saves tokens. |
| **[[1.5.3.7 Auto-Encoders]]** | Neural compression | Latent space mapping, specialized models | Using models trained specifically to compress text into dense representations for other models. |
| **[[1.5.3.8 Lossy vs Lossless]]** | Trade-offs | Precision vs Length, acceptable degradation | Determining how much detail can be sacrificed (lossy) for the sake of fitting in vibration. |
| **[[1.5.3.9 LLMLingua]]** | Research | Prompt compression frameworks, complexity management | Using specialized frameworks designed to compress prompts while maintaining reasoning performance. |
| **[[1.5.3.10 Compression Ratios]]** | Measurement | Original vs Compressed size, efficacy scoring | tracking how effectively a method reduces size without losing core meaning. |

This table represents a **complete guide to Compression Strategies**, the art of saying more with less.
