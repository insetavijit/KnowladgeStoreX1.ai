# 1.5.5 Retrieval-Augmented Generation

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.5.5.1 RAG Overview]]** | Architecture | Retriever, Generator, Knowledge Base, Vector DB | RAG separates memory (database) from reasoning (model), allowing you to search vast libraries. |
| **[[1.5.5.2 Vector Databases]]** | Storage | Embeddings, Pinecone, Milvus, Chroma, approximate neighbor | Storing text as mathematical vectors allows for finding "semantically similarities" rather than just keyword matches. |
| **[[1.5.5.3 Semantic Search]]** | Retrieval mechanism | Cosine similarity, Euclidean distance, dense retrieval | Finding documents that *mean* the same thing as the query, even if they share zero words. |
| **[[1.5.5.4 Chunk Retrieval]]** | Granularity | Fetching paragraphs vs documents, top-k selection | Retrieving just the relevant paragraph is far more efficient than loading the whole 50-page PDF. |
| **[[1.5.5.5 Reranking Results]]** | optimization | Cross-encoders, Cohere Rerank, second-pass scoring | Vector search is fast but imprecise; using a slower "reranker" model on the top results drastically improves quality. |
| **[[1.5.5.6 Context Injection]]** | Prompt engineering | "Answer using this context:", delimiting sources | Effectively placing the retrieved text into the prompt so the model knows to use *only* that data. |
| **[[1.5.5.7 Citations & Sourcing]]** | Trust | Attribution, "According to [1]", source tracking | Forcing the model to cite which retrieved chunk it used prevents hallucinations and builds trust. |
| **[[1.5.5.8 Hybrid Search (Keyword + Vector)]]** | Robustness | BM25 + Dense retrieval, reciprocal rank fusion (RRF) | Combining old-school keyword search (BM25) with vector search covers the blind spots of both. |
| **[[1.5.5.9 Multi-Hop Retrieval]]** | Reasoning | Question decomposition, iterative searching, "HotpotQA" | Some answers require finding fact A, using it to search for fact B, and combining them. |
| **[[1.5.5.10 RAG Evaluation]]** | Metrics | RAGAS, context recall, context precision, answer faithfulness | Measuring "did we find the right document?" separately from "did we write the right answer?" |

This table represents a **complete guide to RAG**, the modern standard for giving LLMs infinite memory.
