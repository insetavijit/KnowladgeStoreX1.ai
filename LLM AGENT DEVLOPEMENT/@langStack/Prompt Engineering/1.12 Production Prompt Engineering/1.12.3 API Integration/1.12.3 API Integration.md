# 1.12.3 API Integration

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.12.3.1 SDK vs Raw HTTP]]** | Implementation | `openai-python` vs `requests.post()` | SDKs handle a lot of edge cases (like retries) for you; use them unless you can't. |
| **[[1.12.3.2 Stream Handling]]** | UX | Server-Sent Events (SSE), chunks, `yield` | Streaming the response token-by-token makes the app feel instant. |
| **[[1.12.3.3 Timeout Configuration]]** | Reliability | "Socket timeout", "Read timeout" | LLMs hang; setting aggressive timeouts prevents your backend from locking up. |
| **[[1.12.3.4 Retry Logic (Exponential Backoff)]]** | Resilience | Jitter, max retries, 429 handling | If the API fails, wait a bit and try again (but don't hammer it). |
| **[[1.12.3.5 Circuit Breakers]]** | Protection | "Stop calling if error rate > 50%" | Failing fast when the upstream provider is down prevents cascading failures. |
| **[[1.12.3.6 Authentication (API Keys)]]** | Security | Key rotation, env vars, vaults | Never commit API keys to Git; inject them at runtime. |
| **[[1.12.3.7 Rate Limiting Handling]]** | Constraints | Token buckets, queuing, backpressure | Managing the flow of requests so you don't hit the vendor's hard limits. |
| **[[1.12.3.8 Request IDs]]** | Tracing | `X-Request-ID`, correlation | Tagging every request so you can find it in the vendor's logs later. |
| **[[1.12.3.9 Connection Pooling]]** | Performance | Keep-alive, reused sockets | Reusing TCP connections reduces the overhead of establishing SSL handshakes. |
| **[[1.12.3.10 Payload Size Limits]]** | Constraints | Max bytes, token estimation | Check input size *before* sending to avoid 400 Bad Request errors. |

This table represents a **complete guide to API Integration**, the plumbing of AI apps.
