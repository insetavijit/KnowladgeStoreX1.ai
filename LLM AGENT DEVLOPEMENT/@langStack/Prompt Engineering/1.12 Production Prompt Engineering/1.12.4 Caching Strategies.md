# 1.12.4 Caching Strategies

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.12.4.1 Exact Match Caching]]** | Basics | Hash(prompt) -> Response | If the user sends the exact same string twice, serve the saved answer. |
| **[[1.12.4.2 Semantic Caching (Embeddings)]]** | Advanced | Vector similarity, thresholding | If the user asks "Hello" and then "Hi", serve the same saved answer. |
| **[[1.12.4.3 TTL (Time To Live)]]** | Freshness | Expiration, "Cache for 24h" | Balacing cost savings with data staleness (don't cache news query forever). |
| **[[1.12.4.4 Cache Invalidation]]** | Hardest problem | "Clear cache on update", eviction | Knowing when to delete a cached item because the underlying data changed. |
| **[[1.12.4.5 Redis/Memcached]]** | Infrastructure | Key-value stores, low latency | The standard tools for storing the cached responses. |
| **[[1.12.4.6 Cost Savings]]** | Metrics | "Cache Hit Rate", $$ saved | Every cache hit is a free API call; maximize hits to minimize bills. |
| **[[1.12.4.7 Latency Reduction]]** | User Exp | 50ms (Cache) vs 2000ms (LLM) | Caching is the only way to get LLM responses in under 100ms. |
| **[[1.12.4.8 Tiered Caching]]** | Architecture | Local (RAM) -> Distributed (Redis) -> API | Checking fast memory first, then shared memory, then the slow model. |
| **[[1.12.4.9 User-Specific Cache]]** | Privacy | Isolating caches by UserID | Ensuring User A doesn't see User B's cached private data. |
| **[[1.12.4.10 Cache Warming]]** | Optimization | Pre-populating common queries | Running the most popular queries at night so they are instant in the morning. |

This table represents a **complete guide to Caching Strategies**, the secret to fast and cheap AI.
