# 1.2.1 Zero-Shot Prompting

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.2.1.1 The Zero-Shot Premise]]** | Core capability | No examples, direct instruction, intrinsic knowledge reliance | Relying entirely on the model's pre-trained knowledge to perform a task without demonstration. |
| **[[1.2.1.2 Instruction Design for Zero-Shot]]** | Clarity | Imperative verbs, distinct sections, removing ambiguity | Without examples to guide it, the model relies 100% on the precision of your instructions. |
| **[[1.2.1.3 Role-Playing in Zero-Shot]]** | Persona adoption | "Act as...", system message grounding, stylistic constraint | Assigning a persona gives the model a behavioral template to fill the void of missing examples. |
| **[[1.2.1.4 Negative Constraints]]** | Boundary setting | Exclusion rules, formatted restrictions, "Do not..." | Explicitly stating what *not* to do is critical when the model has no reference examples to infer boundaries from. |
| **[[1.2.1.5 "Let's Think Step-by-Step" (Zero-Shot CoT)]]** | Reasoning trigger | Magic phrases, Kojima et al. (2022), latent reasoning | Adding a simple trigger phrase can unlock latent reasoning capabilities in zero-shot settings. |
| **[[1.2.1.6 Handling Ambiguity]]** | Robustness | Fallback requests, clarification prompts, "If unsure..." | Instructing the model to ask questions or state uncertainty prevents hallucinations when context is missing. |
| **[[1.2.1.7 Task Description Specificity]]** | Information density | Detailed framing, input/output description, edge case rules | The less you show (examples), the more you must tell (detailed descriptions). |
| **[[1.2.1.8 When Zero-Shot Fails]]** | Limitations | Niche domains, complex formatting, nuances, logical leaps | Zero-shot struggles with tasks that require specific, non-generalizable patterns or strict stylistic adherence. |
| **[[1.2.1.9 Format Enforcement]]** | Structural control | Schema definition, template injection, blank forms | Providing a skeletal structure in the prompt forces the output format even without example data. |
| **[[1.2.1.10 Model Size Dependencies]]** | Capability requirement | Parameter count impact, RLHF tuning, instruction-tuned vs base | Larger, instruction-tuned models (e.g., GPT-4) perform significantly better at zero-shot than smaller models. |

This table represents a **complete guide to Zero-Shot Prompting**, the baseline technique for leveraging pure model intelligence.
