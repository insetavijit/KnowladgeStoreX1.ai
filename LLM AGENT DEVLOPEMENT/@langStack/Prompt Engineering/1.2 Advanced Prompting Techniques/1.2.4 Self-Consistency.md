# 1.2.4 Self-Consistency

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.2.4.1 The Self-Consistency Premise]]** | Ensemble reasoning | Majority voting, intuition stability, error cancellation | If you ask the model to think multiple times, the most common answer is likely the correct one. |
| **[[1.2.4.2 Sampling Strategies]]** | Diversity injection | Temperature > 0.5, Top-P, random seed variation | To get diverse answers for voting, you must sample with non-zero temperature; deterministic sampling (temp=0) fails here. |
| **[[1.2.4.3 Aggregation Methods]]** | Consensus logic | Exact match, fuzzy matching, semantic clustering, LLM-based voting | Grouping similar answers (e.g., "4", "4.0", "four") is critical to accurately counting votes. |
| **[[1.2.4.4 Diversity of Thought]]** | Prompt variation | Paraphrasing prompts, different CoT styles, heterogenous sampling | Encouraging different *ways* of thinking (not just different random seeds) leads to more robust consensus. |
| **[[1.2.4.5 Computational Costs]]** | Resource scaling | N-times inference (e.g., N=10), linear cost increase, latency | Self-consistency requires running the model N times, linearly multiplying cost and latency. |
| **[[1.2.4.6 Confidence Scoring]]** | Probability analysis | Agreement rate (e.g., 9/10), reliability estimation | The degree of consensus (e.g., 90% vs 51%) serves as a proxy for the model's confidence in accurate reasoning. |
| **[[1.2.4.7 Self-Consistency for Code]]** | Execution-based | Unit test voting, execution match, output equivalence | For code, "consistency" means "programs that produce the same output," even if the code itself differs. |
| **[[1.2.4.8 Universal Self-Consistency]]** | Broad application | Fact-checking, creative choice, non-reasoning tasks | While invented for math, consensus can also stabilize open-ended answers by filtering out wild hallucinations. |
| **[[1.2.4.9 Thresholding & Fallbacks]]** | Decision logic | "Unsure" threshold, human handoff, recursive prompting | If no clear majority emerges (e.g., 3-3-4 split), the system should flag uncertainty rather than guessing. |
| **[[1.2.4.10 Marginal Gains Analysis]]** | Optimization | Diminishing returns (N=5 vs N=40), cost-benefit curve | Most gains come from the first few samples (N=5-10); going to N=100 rarely justifies the massive cost. |

This table represents a **complete guide to Self-Consistency**, a brute-force but highly effective method for maximizing reasoning reliability.
