# 1.2.2 Few-Shot Learning

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.2.2.1 The Few-Shot Paradigm]]** | In-Context Learning | Inductive reasoning, pattern recognition, meta-learning | Teaching the model a new task by providing a small set of input-output examples directly in the prompt. |
| **[[1.2.2.2 Example Selection Strategy]]** | Quality control | Representative samples, edge case coverage, clean data | The quality and representativeness of your few-shot examples matter more than the quantity. |
| **[[1.2.2.3 Example Ordering Bias]]** | Bias mitigation | Recency bias, label distribution, random ordering | Models often over-prioritize the last example provided; shuffling or careful ordering mitigates this. |
| **[[1.2.2.4 The K-Shot Tradeoff]]** | Context economics | Token usage vs performance gain, diminishing returns | Increasing examples (K) improves accuracy but consumes context window; finding the optimal K is key. |
| **[[1.2.2.5 Formatting Examples]]** | Syntax clarity | Q: A:, User: Assistant:, separator tokens, xml wrapping | Consistent, clear separation between examples ensures the model understands where one ends and the next begins. |
| **[[1.2.2.6 Label Smoothing]]** | Consistency | Uniform mapping, avoiding synonyms, strict output space | Ensure all examples map similar inputs to identical output formats to prevents stylistic drifting. |
| **[[1.2.2.7 Handling Outliers]]** | Robustness | Out-of-distribution handling, "unknown" labels, error examples | Including examples of how to handle weird or invalid inputs prevents the model from breaking. |
| **[[1.2.2.8 Dynamic Example Selection]]** | Semantic retrieval | RAG for shots, KNN selection, relevance scoring | Dynamically retrieving the most relevant examples for the specific query outperforms static example sets. |
| **[[1.2.2.9 Negative Examples]]** | Boundary reinforcement | "Do not do this", error correction pairs, contrastive learning | Showing the model what *not* to do (and labeling it as such) sharpens its understanding of constraints. |
| **[[1.2.2.10 Few-Shot vs Fine-Tuning]]** | Implementation choice | Iteration speed, data requirement, static vs dynamic | Few-shot is best for rapid prototyping and dynamic tasks; fine-tuning is for fixed, high-volume, specific tasks. |

This table represents a **complete guide to Few-Shot Learning**, the most powerful tool for steering LLM behavior without training.
