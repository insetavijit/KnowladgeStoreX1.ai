# 1.2.8 Generated Knowledge

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.2.8.1 Knowledge Generation Step]]** | Brainstorming | "Write down everything you know about...", fact retrieval | Explicitly asking the model to dump relevant facts *before* answering a question improves accuracy. |
| **[[1.2.8.2 Knowledge Integration]]** | Context injection | Augmenting prompt, combining facts, synthesis | The generated knowledge is then fed back into the prompt as "context" for the final answer. |
| **[[1.2.8.3 Improving Common Sense]]** | Gap bridging | Implicit knowledge explicitation, reasoning support | Helps the model surface "dormant" training data that it might otherwise fail to recall during direct answering. |
| **[[1.2.8.4 Reducing Hallucination]]** | Self-consistency | Fact-checking, self-verification, confidence boost | By explicitly listing facts first, the model is less likely to invent details that contradict its own knowledge base. |
| **[[1.2.8.5 Diversity in Knowledge]]** | Multiple perspectives | Generating multiple knowledge snippets, broad coverage | Exploring different angles of a topic ensures the final answer isn't biased by a single narrow recall path. |
| **[[1.2.8.6 The Two-Step Prompt Pattern]]** | Workflow | Step 1: Generate Knowledge â†’ Step 2: Answer Question | The standard pattern forces a pause for reflection: "Don't just answer; tell me what you know first." |
| **[[1.2.8.7 Domain Adaptation]]** | Priming | Jargon, specific concepts, technical definitions | Generating domain-specific knowledge "primes" the model's latent state to speak in the correct technical register. |
| **[[1.2.8.8 Verification of Generated Knowledge]]** | Reliability | "Is this true?", critique step, filtering | Ideally, an intermediate step should verify that the generated knowledge is actually correct before using it. |
| **[[1.2.8.9 Inference Latency]]** | performance cost | Double inference, increased token count, slower response | Using generated knowledge requires at least two model calls, doubling the latency for the user. |
| **[[1.2.8.10 Comparison with RAG]]** | Retrieval source | Parametric (Internal) vs Non-Parametric (External) | Generated Knowledge uses the model's internal brain; RAG uses an external library. Use GK when external access is impossible. |

This table represents a **complete guide to Generated Knowledge Prompting**, a method to check the model's own homework before answering.
