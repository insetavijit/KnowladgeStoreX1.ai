# 1.2.3 Chain-of-Thought (CoT)

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.2.3.1 CoT Fundamentals]]** | Intermediate reasoning | Step-by-step logic, decompose-and-solve, linear thinking | Forcing the model to "show its work" before answering significantly improves performance on complex tasks. |
| **[[1.2.3.2 Zero-Shot CoT]]** | Prompt engineering | "Let's think step by step", simple trigger, generalized reasoning | A single instruction can trigger reasoning chains even without specific examples (though often less robust). |
| **[[1.2.3.3 Few-Shot CoT]]** | In-context demonstration | Reasoning traces, rationales, manually written steps | Providing examples that include the *thought process* (not just the answer) teaches the model *how* to reason. |
| **[[1.2.3.4 Extracting the Answer]]** | Output parsing | Delimiters, "Therefore, the answer is...", separation | Clearly separating the reasoning block from the final answer enables programmatic extraction of the result. |
| **[[1.2.3.5 CoT for Math & Logic]]** | Symbolic tasks | Arithmetic, symbolic manipulation, logic puzzles | CoT is essential for math, as LLMs are probabilistic text predictors, not calculators; steps bridge the gap. |
| **[[1.2.3.6 CoT for Common Sense]]** | World knowledge | Contextual bridging, implicit assumptions, explanation | CoT helps the model articulate implicit assumptions needed to solve common-sense reasoning problems. |
| **[[1.2.3.7 Avoiding Hallucination with CoT]]** | Self-correction | Fact checking, logical consistency, slowing down | The "time to think" provided by CoT reduces hallucinations by allowing the model to correct its own path. |
| **[[1.2.3.8 CoT Token Costs]]** | Resource management | Verbosity, latency impact, cost vs accuracy trade-off | CoT generates significantly more tokens (reasoning text), increasing both cost and latency. |
| **[[1.2.3.9 Auto-CoT]]** | Scalability | Automated rationale generation, Zhang et al. (2022) | Using the model itself to generate reasoning chains for examples allows for scalable CoT prompt creation. |
| **[[1.2.3.10 When CoT Hurts]]** | Failure modes | Over-thinking, simple tasks, hallucinated steps | For trivial tasks, CoT can introduce unnecessary noise or "hallucinated complexity," degrading performance. |

This table represents a **complete guide to Chain-of-Thought**, the breakthrough technique that unlocked complex reasoning in LLMs.
