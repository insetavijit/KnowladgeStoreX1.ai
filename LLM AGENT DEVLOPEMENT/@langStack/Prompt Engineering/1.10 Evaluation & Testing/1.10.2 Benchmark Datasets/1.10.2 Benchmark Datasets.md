# 1.10.2 Benchmark Datasets

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.10.2.1 MMLU (Massive Multitask)]]** | General knowledge | 57 subjects, STEM, humanities, multiple choice | The current standard for "How smart is this model generally?" |
| **[[1.10.2.2 HellaSwag]]** | Common sense | Sentence completion, physical reasoning | Testing if the model understands how the world works (e.g., "The man drops the glass, it... breaks"). |
| **[[1.10.2.3 TruthfulQA]]** | Honesty | Hallucination detection, misconception handling | Testing if the model mimics human falsehoods or sticks to the truth. |
| **[[1.10.2.4 GSM8K (Math)]]** | Reasoning | Grade school math, multi-step word problems | The standard benchmark for basic mathematical reasoning capability. |
| **[[1.10.2.5 HumanEval (Code)]]** | Programming | Python function completion, unit tests | Testing if the model can write code that actually runs and passes tests. |
| **[[1.10.2.6 Creating Custom Datasets]]** | Specificity | Domain-specific Q&A, Golden sets | Building your own exam is the only way to know if the model works for *your* use case. |
| **[[1.10.2.7 Golden Sets]]** | Ground truth | Perfect answers, reference outputs | A curated list of "Perfect Questions" and "Perfect Answers" to grade against. |
| **[[1.10.2.8 Data Contamination]]** | Cheating | Leakage, memorization vs generalization | Ensuring the model hasn't seen the test questions during its training (memorization). |
| **[[1.10.2.9 Dynamic Benchmarks]]** | Evolving | Canary strings, rolling updates | Tests that change over time to prevent models from overfitting to static benchmarks. |
| **[[1.10.2.10 Domain-Specific Evals]]** | Niche | Legal, Medical, Financial specific benchmarks | specialized tests for specialized fields (e.g., USMLE for doctors). |

This table represents a **complete guide to Benchmark Datasets**, the standardized tests for AI.
