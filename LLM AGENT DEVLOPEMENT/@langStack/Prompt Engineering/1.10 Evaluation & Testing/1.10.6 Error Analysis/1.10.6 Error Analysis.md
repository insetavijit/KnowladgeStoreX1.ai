# 1.10.6 Error Analysis

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.10.6.1 Failure Mode Taxonomy]]** | Classification | Categorizing errors (Hallucination, Refusal, Format) | You can't fix "it's broken"; you can only fix "it hallucinates dates". |
| **[[1.10.6.2 Root Cause Analysis]]** | Debugging | "Why did it fail?", bad retrieval vs bad reasoning | Tracing the error back to the source (e.g., did the RAG step miss the document?). |
| **[[1.10.6.3 Edge Case Identification]]** | Stress testing | "What about empty input?", "What about 1M tokens?" | Finding the weird inputs that break the system logic. |
| **[[1.10.6.4 Confusion Matrices]]** | Visualization | True/False Positives/Negatives grid | Visualizing where the model gets confused (e.g., confusing "Refund" with "Return"). |
| **[[1.10.6.5 Clustering Errors]]** | Patterns | Semantic clustering, "All math errors are similar" | Grouping 1000 failures into 5 distinct topics reveals the biggest fires to put out. |
| **[[1.10.6.6 Visualization Tools]]** | Tooling | Arize Phoenix, LangSmith, scatter plots | Seeing your data in 3D (embeddings) helps spot the semantic clusters of failure. |
| **[[1.10.6.7 Impact Assessment]]** | Severity | Critical vs Minor, "User noticed?" | prioritizing fixes based on how much the error hurts the user experience. |
| **[[1.10.6.8 Prioritization]]** | Triage | Pareto principle (80/20 rule) | Fixing the one error type that causes 80% of the complaints. |
| **[[1.10.6.9 Debugging Workflows]]** | Process | Replay, step-through, prompt isolation | The standard loop: Reproduce the error -> Fix the prompt -> Verify the fix -> Regression test. |
| **[[1.10.6.10 Tracking Fix Rates]]** | Management | "Burndown chart", velocity | Measuring how fast the team is closing known prompt issues. |

This table represents a **complete guide to Error Analysis**, learning from mistakes.
