# 1.10.1 Evaluation Metrics

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.10.1.1 Accuracy vs Perplexity]]** | Core measures | Exact match, statistical surprise, token probability | Perplexity measures how confused the model is; accuracy measures if it's right. |
| **[[1.10.1.2 Precision and Recall]]** | Classification | True positives, false negatives, retrieval quality | Determining if the model found *all* the right answers (Recall) or *only* right answers (Precision). |
| **[[1.10.1.3 F1 Score]]** | Balance | Harmonic mean, balanced metric | A single number summarizing the trade-off between precision and recall. |
| **[[1.10.1.4 BLEU & ROUGE]]** | Text overlap | N-gram matching, translation quality, summarization | Old-school metrics for measuring how much the output text overlaps with a reference text. |
| **[[1.10.1.5 Semantic Similarity]]** | Embeddings | Cosine similarity, vector distance, BertScore | Measuring if the *meaning* is the same, even if the words are different. |
| **[[1.10.1.6 Human-in-the-Loop Metrics]]** | Quality | Likert scale, preference ranking, "Thumbs up" | The gold standard: asking a human "Is this good?" |
| **[[1.10.1.7 Task-Specific KPIs]]** | Business value | Conversion rate, code compilation rate, resolution time | Metrics that actually matter to the business (e.g., "Did the user buy the product?"). |
| **[[1.10.1.8 Latency Metrics]]** | Speed | TTFT (Time to First Token), Total generation time | Measuring how fast the model feels to the user. |
| **[[1.10.1.9 Cost per Evaluation]]** | Economics | Token usage, API bills, ROI | tracking how much money it costs to answer one question. |
| **[[1.10.1.10 Robustness Scores]]** | Stability | Variance across runs, sensitivity to noise | Measuring if the model gives the same answer to the same question twice. |

This table represents a **complete guide to Evaluation Metrics**, the yardsticks of intelligence.
