# 1.10.4 Automated Testing

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.10.4.1 Unit Tests for Prompts]]** | QA | Assertions, expected output, `test_prompt.py` | Treating prompts like code: writing tests that assert specific behaviors. |
| **[[1.10.4.2 Regression Testing]]** | Stability | Back-testing, "Did we break the old features?" | Running old prompts against the new model version to ensure nothing got worse. |
| **[[1.10.4.3 Model-Graded Evals (LLM-as-Judge)]]** | Scaling | GPT-4 grading GPT-3.5, auto-grader | Using a smarter model to grade the outputs of a faster model. |
| **[[1.10.4.4 CI/CD Integration]]** | DevOps | GitHub Actions, pipeline tests, deploy gates | Automatically running prompt tests every time the prompt file is changed in Git. |
| **[[1.10.4.5 Assertions]]** | Checks | `assert "error" not in output`, presence checks | Simple, binary checks: "Did the output contain the word 'Sorry'?" |
| **[[1.10.4.6 Deterministic Checks]]** | Formatting | JSON validity, schema compliance, regex | Verifying things that *must* be true (e.g., proper JSON format) is easy to automate. |
| **[[1.10.4.7 Property-Based Testing]]** | Invariants | "Output length < Input length", logical rules | Testing for properties that should always hold true, regardless of the specific input. |
| **[[1.10.4.8 Test Coverage]]** | Scope | Edge cases, topic coverage, diversity | Ensuring your test suite covers all the different types of user queries. |
| **[[1.10.4.9 Mocking & Stubs]]** | Speed | Mocking LLM calls, testing logic | Testing the surrounding code without actually calling the expensive LLM API. |
| **[[1.10.4.10 Flakiness Handling]]** | Reliability | Retries, probabilistic assertions | Handling the fact that LLM tests are non-deterministic and might fail 1% of the time randomly. |

This table represents a **complete guide to Automated Testing**, bringing engineering discipline to prompt writing.
