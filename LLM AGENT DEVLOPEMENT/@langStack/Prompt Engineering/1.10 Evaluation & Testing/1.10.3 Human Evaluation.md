# 1.10.3 Human Evaluation

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.10.3.1 Annotation Guidelines]]** | Standards | Rubrics, instructions for raters, consistency | You cannot get good human data without telling the humans *exactly* how to grade. |
| **[[1.10.3.2 Inter-Rater Reliability]]** | Consistency | Cohen's Kappa, disagreement rates, consensus | Measuring how often two different humans agree on the same grade. |
| **[[1.10.3.3 Elo Rating Systems]]** | Ranking | Chatbot Arena, pairwise comparison, leaderboard | Ranking models by letting them fight head-to-head and updating their score (like Chess). |
| **[[1.10.3.4 Likert Scales]]** | Grading | 1-5 stars, satisfaction rating | "Rate this answer from 1 (Terrible) to 5 (Perfect)." |
| **[[1.10.3.5 Side-by-Side Comparison]]** | A/B | "Which is better: A or B?", preference | It is easier for humans to pick the winner of two options than to grade one option in isolation. |
| **[[1.10.3.6 Crowdsourcing (MTurk)]]** | Scale | Mechanical Turk, Upwork, scale vs quality | Using paid crowds to get thousands of ratings quickly (but watch out for quality). |
| **[[1.10.3.7 Expert Review]]** | Depth | Domain experts, lawyers, doctors | Expensive but necessary evaluations where random crowds lack the knowledge to judge. |
| **[[1.10.3.8 Bias in Evaluation]]** | Subjectivity | Rater bias, cultural bias, preference for length | acknowledging that humans prefer confident, long answers (even if wrong). |
| **[[1.10.3.9 Feedback Loops]]** | Iteration | Incorporating feedback into training (RLHF) | Closing the loop: Human grades -> Model update -> Better model. |
| **[[1.10.3.10 Cost Management]]** | Budget | Cost per label, efficient sampling | Human evaluation is the most expensive part of the pipeline; optimize it. |

This table represents a **complete guide to Human Evaluation**, the ultimate ground truth.
