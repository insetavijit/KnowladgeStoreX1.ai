# 1.10.7 Performance Monitoring

| **Section** | **Focus** | **Key Utilities / Concepts** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[1.10.7.1 Real-Time Latency]]** | Speed | P95, P99, median, "Time to First Token" | Monitoring how long users are waiting; average is useless, watch the tail (P99). |
| **[[1.10.7.2 Throughput (Tokens/sec)]]** | Volume | Requests per second, token generation speed | Measuring the capacity of the system to handle load. |
| **[[1.10.7.3 Error Rates]]** | Stability | 4xx errors, 5xx errors, refusals | Tracking how often the API fails or the model refuses to answer. |
| **[[1.10.7.4 Cost Tracking]]** | Money | Spend per user, spend per day, token burn | keeping a real-time eye on the credit card bill prevents nasty surprises. |
| **[[1.10.7.5 Drift Detection]]** | Quality | Data drift, concept drift, "Is the model dumber?" | Detecting if the model's behavior has silently changed (e.g., after a vendor update). |
| **[[1.10.7.6 Alerting Thresholds]]** | Ops | PagerDuty, "Latency > 5s", "Error rate > 1%" | Waking up the engineer when the system is actually on fire. |
| **[[1.10.7.7 Usage Spikes]]** | Scaling | Rate limits, autoscaling, "The Viral Effect" | Detecting sudden influxes of traffic before they crash the system. |
| **[[1.10.7.8 Resource Saturation]]** | Infra | GPU memory, queue depth, API quotas | Knowing when you are about to hit the hard limits of your infrastructure. |
| **[[1.10.7.9 User Satisfaction Scores]]** | CSAT | "Did users stop clicking thumbs up?" | A sudden drop in user happiness is the ultimate alarm bell. |
| **[[1.10.7.10 Dashboarding]]** | Visibility | Grafana, Datadog, single pane of glass | Putting all these metrics on one screen so the team can see health at a glance. |

This table represents a **complete guide to Performance Monitoring**, the pulse of the application.
