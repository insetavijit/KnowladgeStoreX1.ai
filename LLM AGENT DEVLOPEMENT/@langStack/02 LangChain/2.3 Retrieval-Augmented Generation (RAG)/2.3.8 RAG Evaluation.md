
| **Subtopic** | **Focus & Purpose** | **Key Concepts / Details** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[2.3.8 RAG Evaluation]]** | Measuring RAG performance | assessing the quality of retrieval and generation | Knowing if your RAG app is actually good or just hallucinating. |
| **[[Retrieval Metrics]]** | Evaluating search | hit rate, MRR (Mean Reciprocal Rank), ndcg | Measuring if the right documents appeared in the search results. |
| **[[Faithfulness]]** | Hallucination checking | ensuring the answer is supported by the retrieved context | Checking if the LLM made stuff up or used the provided text. |
| **[[Answer Relevance]]** | Metric for usefulness | evaluating if the generated answer actually addresses the query | Checking if the AI answered the specific question asked. |
| **[[Context Precision]]** | Signal-to-noise ratio | measuring how much of the retrieved text was actually useful | Checking if we retrieved junk or gold. |
| **[[RAGAS Framework]]** | Automated testing | Retrieval Augmented Generation Assessment, synthetic test data | A standard library for automatically grading RAG pipelines. |
| **[[Evaluation Datasets]]** | Ground truth | creating "golden" QA pairs for regression testing | A list of correct answers to check against. |
