
| **Subtopic** | **Focus & Purpose** | **Key Concepts / Details** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[2.9.8 Prompt Evaluation]]** | QA | quantitatively measuring if a prompt is "good" | Assigning a score to a sentence. |
| **[[Evaluation Metrics]]** | Criteria | defining success (e.g., conciseness, tone match, factual accuracy) | The rubric for grading the AI's homework. |
| **[[Quality Assessment]]** | Review | using an LLM to grade the output of another LLM (LLM-as-a-Judge) | "Teacher GPT grading Student GPT." |
| **[[Consistency Testing]]** | Reliability | checking if the prompt yields similar results over 10 runs | "Does it work every time, or just once?" |
| **[[Regression Testing]]** | Safety | ensuring a new prompt version didn't break old functionality | "Making sure the update didn't introduce bugs." |
| **[[Prompt Benchmarking]]** | Comparison | testing prompts against a standard dataset (e.g., HumanEval) | Running the prompt through a gauntlet. |
| **[[Measuring Effectiveness]]** | ROI | determining if the prompt actually solves the user's problem | "Did the user accept the answer?" |
