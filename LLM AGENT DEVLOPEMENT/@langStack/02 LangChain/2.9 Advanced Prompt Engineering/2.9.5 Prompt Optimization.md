
| **Subtopic** | **Focus & Purpose** | **Key Concepts / Details** | **One-Line Recall** |
| :--- | :--- | :--- | :--- |
| **[[2.9.5 Prompt Optimization]]** | Refinement | systematically improving prompts to get better results | Turning a bronze prompt into a gold prompt. |
| **[[Iterative Refinement]]** | Loop | the cycle of: Draft -> Test -> Fail -> Edit -> Retry | The grind of making it better. |
| **[[A/B Testing]]** | Comparison | running two variations of a prompt against the same dataset | "Which version wins: V1 or V2?" |
| **[[Prompt Metrics]]** | Measurement | defining what "good" looks like (e.g., accuracy, brevity) | Keeping score. |
| **[[DSPy Integration]]** | Automation | using the DSPy library to *compile* prompts instead of writing them | Letting an algorithm write the prompt for you. |
| **[[Automated Optimization]]** | Tuning | using LLMs to rewrite their own prompts (APE - Automatic Prompt Engineering) | "Hey GPT, write a better prompt for yourself." |
| **[[Version Control]]** | Tracking | using Git or LangSmith to track changes to prompt strings | "Revert to the prompt from last Tuesday." |
